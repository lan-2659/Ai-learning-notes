# 八、自动微分

自动微分模块 torch.autograd 负责自动计算张量操作的梯度，具有自动求导功能。自动微分模块是构成神经网络训练的必要模块，可以实现网络权重参数的更新，使得反向传播算法的实现变得简单而高效。

## 1. 基础概念

### 1.1 张量

Torch 中一切皆为张量，属性 `requires_grad` 决定是否对其进行梯度计算。默认是 False，如需计算梯度则设置为 True。

### 1.2 计算图

torch.autograd 通过创建一个动态计算图来跟踪张量的操作，每个张量是计算图中的一个节点，节点之间的操作构成图的边。

在 PyTorch 中，当张量的 requires_grad=True 时，PyTorch 会自动跟踪与该张量相关的所有操作，并构建计算图。每个操作都会生成一个新的张量，并记录其依赖关系。当设置为 `True` 时，表示该张量在计算图中需要参与梯度计算，即在反向传播（Backpropagation）过程中会自动计算其梯度；当设置为 `False` 时，不会计算梯度。

例如：
$$
z = x * y\\loss = z.sum()
$$

在上述代码中，x 和 y 是输入张量，即叶子节点，z 是中间结果，loss 是最终输出。每一步操作都会记录依赖关系：

z = x * y：z 依赖于 x 和 y。

loss = z.sum()：loss 依赖于 z。

这些依赖关系形成了一个动态计算图，如下所示：

```
	  x       y
          \     /
           \   /
            \ /
             z
             |
             |
             v
           loss
```

**叶子节点**：

在 PyTorch 的自动微分机制中，叶子节点（leaf node） 是计算图中：

- 由用户直接创建的张量，并且它的 requires_grad=True。
- 这些张量是计算图的起始点，通常作为模型参数或输入变量。

特征：

- 没有由其他张量通过操作生成。
- 如果参与了计算，其梯度会存储在 leaf_tensor.grad 中。
- 默认情况下，叶子节点的梯度**不会自动清零**，需要显式调用 optimizer.zero_grad() 或 x.grad.zero_() 清除。

**如何判断一个张量是否是叶子节点？**

通过 tensor.is_leaf 属性，可以判断一个张量是否是叶子节点。

```python
x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)  # 叶子节点
y = x ** 2  # 非叶子节点（通过计算生成）
z = y.sum()

print(x.is_leaf)  # True
print(y.is_leaf)  # False
print(z.is_leaf)  # False
```

叶子节点与非叶子节点的区别

| **特性**         | **叶子节点**        | **非叶子节点**                          |
| -------------- | --------------- | ---------------------------------- |
| **创建方式**       | 用户直接创建的张量       | 通过其他张量的运算生成                        |
| **is_leaf 属性** | True            | False                              |
| **梯度存储**       | 梯度存储在 .grad 属性中 | 梯度不会存储在 .grad，只能通过反向传播传递           |
| **是否参与计算图**    | 是计算图的起点         | 是计算图的中间或终点                         |
| **删除条件**       | 默认不会被删除         | 在反向传播后，默认被释放（除非 retain_graph=True） |

`detach()`：张量 x 从计算图中分离出来，返回一个新的张量，与 x 共享数据，但**不包含计算图**（即不会追踪梯度）。

**特点**：

- 返回的张量是一个新的张量，与原始张量共享数据。
- 对 x.detach() 的操作不会影响原始张量的梯度计算。
- 推荐使用 detach()，因为它更安全，且在未来版本的 PyTorch 中可能会取代 data。

```python
x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)
y = x.detach()  # y 是一个新张量，不追踪梯度

y += 1  # 修改 y 不会影响 x 的梯度计算
print(x)  # tensor([1., 2., 3.], requires_grad=True)
print(y)  # tensor([2., 3., 4.])
```

### 1.3 反向传播

使用 tensor.backward() 方法执行反向传播，从而计算张量的梯度。这个过程会自动计算每个张量对损失函数的梯度。例如：调用 loss.backward() 从输出节点 loss 开始，沿着计算图反向传播，计算每个节点的梯度。

### 1.4 梯度

计算得到的梯度通过 tensor.grad 访问，这些梯度用于优化模型参数，以最小化损失函数。

## 2. 计算梯度

使用 tensor.backward() 方法执行反向传播，从而计算张量的梯度

### 2.1 标量梯度计算

参考代码如下：

```python
import torch

def test001():
    # 1. 创建张量：必须为浮点类型
    x = torch.tensor(1.0, requires_grad=True)
    
    # 2. 操作张量
    y = x ** 2

    # 3. 计算梯度，也就是反向传播
    y.backward()

    # 4. 读取梯度值
    print(x.grad)  # 输出: tensor(2.)

if __name__ == "__main__":
    test001()
```

### 2.2 向量梯度计算

案例：

```python
def test003():
    # 1. 创建张量：必须为浮点类型
    x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)

    # 2. 操作张量
    y = x ** 2

    # 3. 计算梯度，也就是反向传播
    y.backward()

    # 4. 读取梯度值
    print(x.grad)
```

错误预警：RuntimeError: grad can be implicitly created only for scalar outputs

由于 *y* 是一个向量，我们需要提供一个与 *y* 形状相同的向量作为 backward() 的参数，这个参数通常被称为 **梯度张量**（gradient tensor），它表示 *y* 中每个元素的梯度。

```python
def test003():
    # 1. 创建张量：必须为浮点类型
    x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)

    # 2. 操作张量
    y = x ** 2

    # 3. 计算梯度，也就是反向传播
    y.backward(torch.tensor([1.0, 1.0, 1.0]))

    # 4. 读取梯度值
    print(x.grad)
    
    # 输出
    # tensor([2., 4., 6.])

if __name__ == "__main__":
    test003()
```

我们也可以将向量 *y* 通过一个标量损失函数（如 y.mean()）转换为一个标量，反向传播时就不需要提供额外的梯度向量参数了。这是因为标量的梯度是明确的，直接调用 .backward() 即可。

```python
import torch

def test002():
    # 1. 创建张量：必须为浮点类型
    x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)

    # 2. 操作张量
    y = x ** 2

    # 3. 损失函数
    loss = y.mean()

    # 4. 计算梯度，也就是反向传播
    loss.backward()

    # 5. 读取梯度值
    print(x.grad)

if __name__ == "__main__":
    test002()
```

调用 loss.backward() 从输出节点 loss 开始，沿着计算图反向传播，计算每个节点的梯度。

损失函数$$loss=mean(y)=\frac{1}{n}∑_{i=1}^ny_i$$，其中 *n*=3。

对于每个 $y_i$，其梯度为 $\frac{∂loss}{∂y_i}=\frac{1}{n}=\frac13$。

对于每个 $x_i$，其梯度为：
$$
\frac{∂loss}{∂x_i}=\frac{∂loss}{∂y_i}×\frac{∂y_i}{∂x_i}=\frac1{3}×2x_i=\frac{2x_i}3
$$

所以，x.grad 的值为：$[\frac{2×1.0}3, \frac{2×2.0}3, \frac{2×3.0}3]=[\frac23,\frac43,2]≈[0.6667,1.3333,2.0000]$

### 2.3 梯度传播的数学原理

PyTorch 使用链式法则进行梯度传播。考虑复合函数：
$$
y = f(g(x))
$$

梯度计算为：
$$
\frac{dy}{dx} = \frac{dy}{dg} \cdot \frac{dg}{dx}
$$

在向量情况下，这表现为雅可比矩阵乘法：
$$
\frac{\partial \mathbf{y}}{\partial \mathbf{x}} = \frac{\partial \mathbf{y}}{\partial \mathbf{z}} \cdot \frac{\partial \mathbf{z}}{\partial \mathbf{x}}
$$

其中 $\frac{\partial \mathbf{y}}{\partial \mathbf{z}}$ 是雅可比矩阵。

### 2.4 多标量梯度计算

参考代码如下

```python
import torch

def test003():
    # 1. 创建两个标量
    x1 = torch.tensor(5.0, requires_grad=True, dtype=torch.float64)
    x2 = torch.tensor(3.0, requires_grad=True, dtype=torch.float64)

    # 2. 构建运算公式
    y = x1**2 + 2 * x2 + 7
    
    # 3. 计算梯度，也就是反向传播
    y.backward()
    
    # 4. 读取梯度值
    print(x1.grad, x2.grad)
    
    # 输出：
    # tensor(10., dtype=torch.float64) tensor(2., dtype=torch.float64)

if __name__ == "__main__":
    test003()
```

### 2.5 多向量梯度计算

代码参考如下

```python
import torch

def test004():
    # 创建两个张量，并设置 requires_grad=True
    x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)
    y = torch.tensor([4.0, 5.0, 6.0], requires_grad=True)

    # 前向传播：计算 z = x * y
    z = x * y

    # 前向传播：计算 loss = z.sum()
    loss = z.sum()

    # 查看前向传播的结果
    print("z:", z)  # 输出: tensor([ 4., 10., 18.], grad_fn=<MulBackward0>)
    print("loss:", loss)  # 输出: tensor(32., grad_fn=<SumBackward0>)

    # 反向传播：计算梯度
    loss.backward()

    # 查看梯度
    print("x.grad:", x.grad)  # 输出: tensor([4., 5., 6.])
    print("y.grad:", y.grad)  # 输出: tensor([1., 2., 3.])

if __name__ == "__main__":
    test004()
```

## 3. 梯度上下文控制

梯度计算的上下文控制和设置对于管理计算图、内存消耗、以及计算效率至关重要。下面我们学习下 Torch 中与梯度计算相关的一些主要设置方式。

### 3.1 控制梯度计算

梯度计算是有性能开销的，有些时候我们只是简单的运算，并不需要梯度

```python
import torch

def test001():
    x = torch.tensor(10.5, requires_grad=True)
    print(x.requires_grad)  # True

    # 1. 默认 y 的 requires_grad=True
    y = x**2 + 2 * x + 3
    print(y.requires_grad)  # True

    # 2. 如果不需要 y 计算梯度 - with 进行上下文管理
    with torch.no_grad():
        y = x**2 + 2 * x + 3
    print(y.requires_grad)  # False

    # 3. 如果不需要 y 计算梯度 - 使用装饰器
    @torch.no_grad()
    def y_fn(x):
        return x**2 + 2 * x + 3

    y = y_fn(x)
    print(y.requires_grad)  # False

    # 4. 如果不需要 y 计算梯度 - 全局设置，需要谨慎
    torch.set_grad_enabled(False)
    y = x**2 + 2 * x + 3
    print(y.requires_grad)  # False

if __name__ == "__main__":
    test001()
```

### 3.2 累计梯度

默认情况下，当我们重复对一个自变量进行梯度计算时，梯度是累加的

```python
import torch

def test002():
    # 1. 创建张量：必须为浮点类型
    x = torch.tensor([1.0, 2.0, 5.3], requires_grad=True)

    # 2. 累计梯度：每次计算都会累计梯度
    for i in range(3):
        y = x**2 + 2 * x + 7
        z = y.mean()
        z.backward()
        print(x.grad)

if __name__ == "__main__":
    test002()
```

输出结果：

```python
tensor([1.3333, 2.0000, 4.2000])
tensor([2.6667, 4.0000, 8.4000])
tensor([ 4.0000,  6.0000, 12.6000])
```

思考：如果把 `y = x**2 + 2 * x + 7` 放在循环外，会是什么结果？

会报错：

```
RuntimeError: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.
```

PyTorch 的自动求导机制在调用 `backward()` 时，会计算梯度并将中间结果存储在计算图中。默认情况下，这些中间结果在第一次调用 `backward()` 后会被释放，以节省内存。如果再次调用 `backward()`，由于中间结果已经被释放，就会抛出这个错误。

### 3.3 梯度清零

大多数情况下是不需要梯度累加的，奇葩的事情还是需要解决的~

```python
import torch

def test002():
    # 1. 创建张量：必须为浮点类型
    x = torch.tensor([1.0, 2.0, 5.3], requires_grad=True)

    # 2. 累计梯度：每次计算都会累计梯度
    for i in range(3):
        y = x**2 + 2 * x + 7
        z = y.mean()
        # 2.1 反向传播之前先对梯度进行清零
        if x.grad is not None:
            x.grad.zero_()
            
        z.backward()
        print(x.grad)

if __name__ == "__main__":
    test002()
    
# 输出：
# tensor([1.3333, 2.0000, 4.2000])
# tensor([1.3333, 2.0000, 4.2000])
# tensor([1.3333, 2.0000, 4.2000])
```

### 3.4 计算图管理

PyTorch 默认在反向传播后自动释放计算图，以节省内存。如果需要多次反向传播（如高阶导数计算），需设置 `retain_graph=True`：

```python
# 第一次反向传播，保留计算图
loss.backward(retain_graph=True)

# 第二次反向传播（如计算二阶导数）
loss.backward()
```

但要注意，保留计算图会增加内存消耗，应谨慎使用。

### 3.5 案例1-求函数最小值

通过梯度下降找到函数最小值

```python
import torch
from matplotlib import pyplot as plt
import numpy as np

def test01():
    x = np.linspace(-10, 10, 100)
    y = x ** 2
    plt.plot(x, y)
    plt.show()

def test02():
    # 初始化自变量 X
    x = torch.tensor([3.0], requires_grad=True, dtype=torch.float)
    # 迭代轮次
    epochs = 50
    # 学习率
    lr = 0.1

    list = []
    for i in range(epochs):
        # 计算函数表达式
        y = x ** 2

        # 梯度清零
        if x.grad is not None:
            x.grad.zero_()
        # 反向传播
        y.backward()
        # 梯度下降
        with torch.no_grad():
            # 使用 .data 避免叶子节点被重新赋值
            x.data -= lr * x.grad

        print('epoch:', i, 'x:', x.item(), 'y:', y.item())
        list.append((x.item(), y.item()))

    # 散点图，观察收敛效果
    x_list = [l[0] for l in list]
    y_list = [l[1] for l in list]

    plt.scatter(x=x_list, y=y_list)
    plt.show()

if __name__ == "__main__":
    test01()
    test02()
```

代码解释：

```python
# 梯度下降
with torch.no_grad():
    x.data -= lr * x.grad
```

如果去掉梯度控制会有什么结果？

代码中去掉梯度控制会报异常：

```
RuntimeError: a leaf Variable that requires grad is being used in an in-place operation.
```

因为代码中 x 是叶子节点(叶子张量)，是计算图的开始节点，并且设置需要梯度。在 PyTorch 中不允许对**需要梯度**的叶子变量进行原地操作。因为这会破坏计算图，导致梯度计算错误。

**解决方法**

为了避免这个错误，可以使用以下方法：

1. **使用 .data 属性**：通过 x.data 访问张量的数据部分（不涉及梯度计算），然后进行原地操作。
   ```python
   x.data -= lr * x.grad
   ```

2. **使用 detach()**：创建一个不追踪梯度的副本：
   ```python
   with torch.no_grad():
       new_x = x.detach() - lr * x.grad
       x.data = new_x
   ```

### 3.6 案例2-函数参数求解

```python
import torch

def test02():
    # 定义数据
    x = torch.tensor([1, 2, 3, 4, 5], dtype=torch.float)
    y = torch.tensor([3, 5, 7, 9, 11], dtype=torch.float)

    # 定义模型参数 a 和 b，并初始化
    a = torch.tensor([1], dtype=torch.float, requires_grad=True)
    b = torch.tensor([1], dtype=torch.float, requires_grad=True)
    # 学习率
    lr = 0.01  # 修正为更小的学习率
    # 迭代轮次
    epochs = 100

    for epoch in range(epochs):
        # 前向传播：计算预测值 y_pred
        y_pred = a * x + b

        # 定义损失函数
        loss = ((y_pred - y) ** 2).mean()

        if a.grad is not None and b.grad is not None:
            a.grad.zero_()
            b.grad.zero_()

        # 反向传播：计算梯度
        loss.backward()

        # 梯度下降
        with torch.no_grad():
            a.data -= lr * a.grad
            b.data -= lr * b.grad

        if (epoch + 1) % 10 == 0:
            print(f'Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.4f}')

    print(f'a: {a.item()}, b: {b.item()}')

if __name__ == '__main__':
    test02()
```

**代码逻辑：**

在 PyTorch 中，所有的张量操作都会被记录在一个计算图中。对于代码：

```python
y_pred = a * x + b
loss = ((y_pred - y) ** 2).mean()
```

计算图如下：

```
a → y_pred → loss
x ↗
b ↗
```

- a 和 b 是需要计算梯度的叶子张量（requires_grad=True）。
- y_pred 是中间结果，依赖于 a 和 b。
- loss 是最终的标量输出，依赖于 y_pred。

当调用 loss.backward() 时，PyTorch 会从 loss 开始，沿着计算图反向传播，计算 loss 对每个需要梯度的张量（如 a 和 b）的梯度。

1. 计算 loss 对 y_pred 的梯度：
$$
loss = ((y_pred - y)^ 2).mean()=\frac{1}{n}\Sigma_{i=1}^n(y\_pred_i - y_i)^2
$$

求损失函数关于 *y*_pred 的梯度（即偏导数组成的向量）。由于 loss 是 y_pred 的函数，我们需要对每个 $y\_pred_i$ 求偏导数，并将它们组合成一个向量。

应用链式法则和常数求导规则，对于每个 $(y\_pred_i−y_i)^2$ 项，梯度向量的每个分量是：
$$
\frac{∂loss}{∂y\_pred_i} = \frac{2}{n} (y\_pred_i−y_i)
$$

将结果组合成一个向量，我们得到：
$$
\frac{∂loss}{∂y\_pred} = [\frac{2}{n} (y\_pred_1−y_1), \frac{2}{n} (y\_pred_2−y_2),...,\frac{2}{n} (y\_pred_n−y_n)]\\
=\frac{2}{n} (y\_pred−y)
$$

其中 n=5，y_pred 和 y 均为向量。

2. 计算 y_pred 对 a 和 b 的梯度：

```
y_pred = a * x + b
```

对 a 求导：$\frac{∂y_pred}{∂a}=x$，x 为向量

对 b 求导：$\frac{∂y_pred}{∂b}=1$

3. 根据链式法则，loss 对 a 的梯度为：
$$
\frac{∂loss}{∂a}=\frac{∂loss}{∂y\_pred}⋅\frac{∂y\_pred}{∂a} = \frac{2}{n} (y\_pred−y)x
$$

loss 对 b 的梯度为：
$$
\frac{∂loss}{∂b}=\frac{∂loss}{∂y\_pred}⋅\frac{∂y\_pred}{∂b} = \frac{2}{n} (y\_pred−y)
$$

代码运行结果：

```python
Epoch [10/100], Loss: 0.0965
Epoch [20/100], Loss: 0.0110
Epoch [30/100], Loss: 0.0099
Epoch [40/100], Loss: 0.0092
Epoch [50/100], Loss: 0.0086
Epoch [60/100], Loss: 0.0081
Epoch [70/100], Loss: 0.0075
Epoch [80/100], Loss: 0.0071
Epoch [90/100], Loss: 0.0066
Epoch [100/100], Loss: 0.0062
a: 1.9492162466049194, b: 1.1833451986312866
```

可以看出 loss 损失函数值在收敛，a 接近 2，b 接近 1

将 epochs=500

代码运行结果：

```python
Epoch [440/500], Loss: 0.0006
Epoch [450/500], Loss: 0.0006
Epoch [460/500], Loss: 0.0005
Epoch [470/500], Loss: 0.0005
Epoch [480/500], Loss: 0.0005
Epoch [490/500], Loss: 0.0004
Epoch [500/500], Loss: 0.0004
a: 1.986896276473999, b: 1.0473089218139648
```

a 已经无限接近 2，b 无限接近 1

## 4. 高阶导数

PyTorch 支持高阶导数计算，通过多次反向传播实现：

```python
x = torch.tensor(2.0, requires_grad=True)
y = x**3

# 计算一阶导数
dy_dx = torch.autograd.grad(y, x, create_graph=True)[0]
print("一阶导数:", dy_dx.item())  # 12 (3*2²)

# 计算二阶导数
d2y_dx2 = torch.autograd.grad(dy_dx, x)[0]
print("二阶导数:", d2y_dx2.item())  # 12 (6*2)
```

注意：
1. 计算高阶导数时需设置 `create_graph=True`
2. 高阶导数计算会显著增加内存消耗

## 5. 自定义梯度函数

对于特殊操作，可通过继承 `torch.autograd.Function` 自定义前向传播和反向传播：

```python
class MyReLU(torch.autograd.Function):
    @staticmethod
    def forward(ctx, input):
        ctx.save_for_backward(input)
        return input.clamp(min=0)
    
    @staticmethod
    def backward(ctx, grad_output):
        input, = ctx.saved_tensors
        grad_input = grad_output.clone()
        grad_input[input < 0] = 0
        return grad_input

# 使用自定义函数
x = torch.tensor([-1.0, 2.0], requires_grad=True)
y = MyReLU.apply(x)
y.sum().backward()
print(x.grad)  # tensor([0., 1.])
```

应用场景：
- 实现特殊激活函数
- 优化数值稳定性
- 加速特定计算

## 6. 梯度检查

实现自定义梯度函数后，应验证梯度计算的正确性：

```python
def grad_check():
    x = torch.tensor([1.5], requires_grad=True)
    
    # 解析梯度
    analytic_grad = torch.autograd.grad(MyReLU.apply(x), x)[0]
    
    # 数值梯度
    eps = 1e-5
    f_plus = MyReLU.apply(x + eps)
    f_minus = MyReLU.apply(x - eps)
    numeric_grad = (f_plus - f_minus) / (2 * eps)
    
    # 比较差异
    diff = torch.abs(analytic_grad - numeric_grad)
    print(f"解析梯度: {analytic_grad.item()}, 数值梯度: {numeric_grad.item()}, 差异: {diff.item()}")
```

可接受差异通常在 1e-7 量级以下。