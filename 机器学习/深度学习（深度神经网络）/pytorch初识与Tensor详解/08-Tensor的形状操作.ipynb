{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c8a669",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "在 PyTorch 中，张量的形状操作是非常重要的，因为它允许你灵活地调整张量的维度和结构，以适应不同的计算需求。\n",
    "一般我们会用以下两种方法来操作张量的形状：\n",
    "\n",
    "    view()：\n",
    "        返回的是原始张量视图，不重新分配内存，效率更高\n",
    "        高效，但需要张量在内存中是连续的(如果不连续会报错)\n",
    "\n",
    "    reshape()：\n",
    "        可以用于将张量转换为不同的形状，但要确保转换后的形状与原始形状具有相同的元素数量。\n",
    "        更灵活，但涉及内存复制，效率较低\n",
    "\n",
    "        \n",
    "可以使用 Tensor.is_contiguous() 检查张量在内存中是否连续存储\n",
    "    无需传参，该方法的返回值为布尔值\n",
    "可以使用 Tensor.contiguous() 创建连续副本\n",
    "    无需传参，该方法返回一个连续副本\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85a5f19a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正常情况下的张量： True\n",
      "进行view改变形状：\n",
      " tensor([[1, 2],\n",
      "        [3, 4],\n",
      "        [5, 6]])\n",
      "转置操作的张量： False\n",
      "tensor([[1, 4],\n",
      "        [2, 5],\n",
      "        [3, 6]])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 14\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(tensor)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# 此时使用view进行变形操作\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m tensor \u001b[38;5;241m=\u001b[39m \u001b[43mtensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(tensor)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead."
     ]
    }
   ],
   "source": [
    "\"\"\"view()方法\"\"\"\n",
    "\n",
    "import torch\n",
    "\n",
    "tensor = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "print(\"正常情况下的张量：\", tensor.is_contiguous())\n",
    "print('进行view改变形状：\\n', tensor.view(3, 2))\n",
    "\n",
    "# 对张量进行转置操作\n",
    "tensor = tensor.t()\n",
    "print(\"转置操作的张量：\", tensor.is_contiguous())\n",
    "print(tensor)\n",
    "# 此时使用view进行变形操作\n",
    "tensor = tensor.view(2, -1)\n",
    "print(tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a198eac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"reshape()方法\"\"\"\n",
    "\n",
    "import torch\n",
    "\n",
    "data = torch.randint(0, 10, (4, 3))\n",
    "print(data)\n",
    "# 1. 使用reshape改变形状\n",
    "data = data.reshape(2, 2, 3)\n",
    "print(data)\n",
    "\n",
    "# 2. 使用-1表示自动计算\n",
    "data = data.reshape(2, -1)\n",
    "print(data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86a1382",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "交换张量的维度：\n",
    "    torch.transpose(tensor, dim0, dim1)\n",
    "        用于交换张量的两个维度，返回原张量的视图(view)，不复制数据。\n",
    "        注意：仅能交换两个维度，适用于简单转置操作。\n",
    "        底层：通过调整stride实现，时间复杂度O(1)。\n",
    "\n",
    "    tensor.permute(*dims)  *常用*\n",
    "        重新排列所有维度的顺序，返回新视图(view)，不复制数据。\n",
    "        特点：可以一次性重排多个维度，更灵活。\n",
    "        底层：同样通过调整stride实现，时间复杂度O(1)。\n",
    "        \n",
    "    共同点：\n",
    "        1. 都是视图操作(view)，不复制数据\n",
    "        2. 都返回非连续(non-contiguous)张量\n",
    "        3. 如果后续需要连续内存，需调用.contiguous()    \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "855a5680",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 原始张量 ===\n",
      "形状: torch.Size([2, 3, 4])\n",
      "步长: (12, 4, 1)\n",
      "连续: True\n",
      "数据:\n",
      " tensor([[[ 0,  1,  2,  3],\n",
      "         [ 4,  5,  6,  7],\n",
      "         [ 8,  9, 10, 11]],\n",
      "\n",
      "        [[12, 13, 14, 15],\n",
      "         [16, 17, 18, 19],\n",
      "         [20, 21, 22, 23]]])\n",
      "\n",
      "=== transpose(0,2)结果 ===\n",
      "形状: torch.Size([4, 3, 2])\n",
      "步长: (1, 4, 12)\n",
      "连续: False\n",
      "共享数据: True\n",
      "转置后数据:\n",
      " tensor([[[ 0, 12],\n",
      "         [ 4, 16],\n",
      "         [ 8, 20]],\n",
      "\n",
      "        [[ 1, 13],\n",
      "         [ 5, 17],\n",
      "         [ 9, 21]],\n",
      "\n",
      "        [[ 2, 14],\n",
      "         [ 6, 18],\n",
      "         [10, 22]],\n",
      "\n",
      "        [[ 3, 15],\n",
      "         [ 7, 19],\n",
      "         [11, 23]]])\n",
      "\n",
      "=== permute(2,1,0)结果 ===\n",
      "形状: torch.Size([4, 3, 2])\n",
      "步长: (1, 4, 12)\n",
      "连续: False\n",
      "共享数据: True\n",
      "\n",
      "=== 比较结果 ===\n",
      "transposed和permuted形状相同: True\n",
      "transposed和permuted数据相等: True\n",
      "\n",
      "=== 连续化操作 ===\n",
      "连续化后内存地址: True\n",
      "连续化后是否连续: True\n",
      "\n",
      "=== 复杂permute示例 ===\n",
      "新形状: torch.Size([4, 2, 3])\n",
      "新步长: (1, 12, 4)\n",
      "重排后数据:\n",
      " tensor([[[ 0,  4,  8],\n",
      "         [12, 16, 20]],\n",
      "\n",
      "        [[ 1,  5,  9],\n",
      "         [13, 17, 21]],\n",
      "\n",
      "        [[ 2,  6, 10],\n",
      "         [14, 18, 22]],\n",
      "\n",
      "        [[ 3,  7, 11],\n",
      "         [15, 19, 23]]])\n",
      "\n",
      "=== 在非连续张量上使用view ===\n",
      "错误信息: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.\n",
      "解决方案: 先调用.contiguous()\n",
      "修复后:\n",
      " tensor([[ 0, 12,  4, 16,  8, 20],\n",
      "        [ 1, 13,  5, 17,  9, 21],\n",
      "        [ 2, 14,  6, 18, 10, 22],\n",
      "        [ 3, 15,  7, 19, 11, 23]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\26595\\AppData\\Local\\Temp\\ipykernel_22820\\162467243.py:21: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  print(f\"共享数据: {transposed.storage().data_ptr() == original.storage().data_ptr()}\")  # True\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "torch.transpose 和 torch.permute 示例\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "\n",
    "# 创建原始张量 (3D: 批次×高度×宽度)\n",
    "original = torch.arange(24).reshape(2, 3, 4)  # 形状 [2, 3, 4]\n",
    "print(\"=== 原始张量 ===\")\n",
    "print(f\"形状: {original.shape}\")\n",
    "print(f\"步长: {original.stride()}\")  # (12, 4, 1)\n",
    "print(f\"连续: {original.is_contiguous()}\")\n",
    "print(\"数据:\\n\", original)\n",
    "\n",
    "# 使用transpose交换维度0和2 (批次和宽度)\n",
    "transposed = torch.transpose(original, 0, 2)  # 形状变为 [4, 3, 2]\n",
    "print(\"\\n=== transpose(0,2)结果 ===\")\n",
    "print(f\"形状: {transposed.shape}\")\n",
    "print(f\"步长: {transposed.stride()}\")  # (1, 4, 12) - 步长改变\n",
    "print(f\"连续: {transposed.is_contiguous()}\")  # False\n",
    "print(f\"共享数据: {transposed.storage().data_ptr() == original.storage().data_ptr()}\")  # True\n",
    "print(\"转置后数据:\\n\", transposed)\n",
    "\n",
    "# 使用permute重排所有维度 (新顺序: 宽度×高度×批次)\n",
    "permuted = original.permute(2, 1, 0)  # 形状变为 [4, 3, 2]\n",
    "print(\"\\n=== permute(2,1,0)结果 ===\")\n",
    "print(f\"形状: {permuted.shape}\")\n",
    "print(f\"步长: {permuted.stride()}\")  # (1, 4, 12) - 与transposed相同\n",
    "print(f\"连续: {permuted.is_contiguous()}\")  # False\n",
    "print(f\"共享数据: {permuted.storage().data_ptr() == original.storage().data_ptr()}\")  # True\n",
    "\n",
    "# 验证transpose和permute结果是否相同\n",
    "print(\"\\n=== 比较结果 ===\")\n",
    "print(f\"transposed和permuted形状相同: {transposed.shape == permuted.shape}\")\n",
    "print(f\"transposed和permuted数据相等: {torch.equal(transposed, permuted)}\")  # True\n",
    "\n",
    "# 连续化操作的影响\n",
    "print(\"\\n=== 连续化操作 ===\")\n",
    "contiguous_transposed = transposed.contiguous()\n",
    "print(f\"连续化后内存地址: {contiguous_transposed.storage().data_ptr() != original.storage().data_ptr()}\")  # True\n",
    "print(f\"连续化后是否连续: {contiguous_transposed.is_contiguous()}\")  # True\n",
    "\n",
    "# 更复杂的permute示例 (重排多个维度)\n",
    "print(\"\\n=== 复杂permute示例 ===\")\n",
    "# 原始形状 [2,3,4] -> 新顺序 [2,0,1] 解释:\n",
    "# 新维度0 = 原维度2 (最内层)\n",
    "# 新维度1 = 原维度0 (批次)\n",
    "# 新维度2 = 原维度1 (高度)\n",
    "complex_perm = original.permute(2, 0, 1)  # 形状 [4,2,3]\n",
    "print(f\"新形状: {complex_perm.shape}\")  # [4,2,3]\n",
    "print(f\"新步长: {complex_perm.stride()}\")  # (1,12,4)\n",
    "print(\"重排后数据:\\n\", complex_perm)\n",
    "\n",
    "# 尝试在非连续张量上使用view (会报错)\n",
    "try:\n",
    "    print(\"\\n=== 在非连续张量上使用view ===\")\n",
    "    transposed.view(4, 6)  # 会报错\n",
    "except RuntimeError as e:\n",
    "    print(f\"错误信息: {e}\")\n",
    "    print(\"解决方案: 先调用.contiguous()\")\n",
    "    fixed = transposed.contiguous().view(4, 6)\n",
    "    print(\"修复后:\\n\", fixed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42118367",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "在后续的网络学习中，升维和降维是常用操作，需要掌握。\n",
    "\n",
    "    Tensor.unsqueeze(dim: int)\n",
    "        用于在指定维度前插入一个大小为 1 的新维度。\n",
    "        (如果dim=-1，则会将新维度插入在末尾)\n",
    "    Tensor.squeeze(dim=None)\n",
    "        用于移除所有大小为 1 的维度，或者移除指定维度的大小为 1 的维度。\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a65cb61",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Tensor.unsqueeze 和 Tensor.squeeze 方法示例\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "\n",
    "# 原始 2D 张量 (3x4 矩阵)\n",
    "t = torch.tensor([[1, 2, 3, 4],\n",
    "                 [5, 6, 7, 8],\n",
    "                 [9, 10, 11, 12]])\n",
    "print(\"原始形状:\", t.shape)  # torch.Size([3, 4])\n",
    "\n",
    "# ========== 升维操作 (unsqueeze) ==========\n",
    "# 在维度0插入新维度 (最外层)\n",
    "t1 = t.unsqueeze(0)\n",
    "print(\"\\nunsqueeze(0) 后:\", t1.shape)  # torch.Size([1, 3, 4])\n",
    "\n",
    "# 在维度1插入新维度 (行与列之间)\n",
    "t2 = t.unsqueeze(1)\n",
    "print(\"unsqueeze(1) 后:\", t2.shape)  # torch.Size([3, 1, 4])\n",
    "\n",
    "# 在最后插入新维度 (dim=-1)\n",
    "t3 = t.unsqueeze(-1)\n",
    "print(\"unsqueeze(-1) 后:\", t3.shape)  # torch.Size([3, 4, 1])\n",
    "\n",
    "# ========== 降维操作 (squeeze) ==========\n",
    "# 创建含单值维度的张量\n",
    "t4 = torch.zeros(2, 1, 3, 1, 4)  # 形状: [2,1,3,1,4]\n",
    "print(\"\\n原始含单值维度:\", t4.shape)\n",
    "\n",
    "# 默认移除所有单值维度\n",
    "t5 = t4.squeeze()\n",
    "print(\"squeeze() 后:\", t5.shape)  # torch.Size([2, 3, 4])\n",
    "\n",
    "# 仅移除指定维度 (dim=1)\n",
    "t6 = t4.squeeze(1)\n",
    "print(\"squeeze(dim=1) 后:\", t6.shape)  # torch.Size([2, 3, 1, 4])\n",
    "\n",
    "# 尝试移除非单值维度 (无变化)\n",
    "t7 = t5.squeeze(0)  # 第0维是2(非1)\n",
    "print(\"尝试移除非单值维度:\", t7.shape)  # 仍为 torch.Size([2, 3, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35458f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Tensor也有广播机制，与numpy相同\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "universal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
