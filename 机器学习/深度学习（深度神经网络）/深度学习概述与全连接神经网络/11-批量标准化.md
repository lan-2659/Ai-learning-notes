---
typora-root-url: ..\media
---

# 批量标准化

<img src="/image-20240525080644109.png" alt="image-20240525080644109" style="zoom:42%;" />

批量标准化（Batch Normalization，简称 BN）是神经网络训练中的一项关键技术，它通过**规范化每一层输入的分布（均值为0，方差为1）**来显著提升训练的稳定性、速度和性能。

在深度学习中，批量标准化（Batch Normalization）在**训练阶段**和**测试阶段**的行为是不同的。在测试阶段，由于没有 mini-batch 数据，无法直接计算当前 batch 的均值和方差，因此需要使用训练阶段计算的**全局统计量**（均值和方差）来进行标准化。

官网地址：https://pytorch.org/docs/stable/nn.html#normalization-layers

## 1. 训练阶段的批量标准化

### 1.1 计算均值和方差

对于给定的神经网络层，假设输入数据为 $$\mathbf{x} = \{x_1, x_2, \ldots, x_m\}$$，其中 $$m是$$批次大小。我们首先计算该**批次**数据的均值和方差。

- 均值（Mean）
  $$
  \mu_B = \frac{1}{m} \sum_{i=1}^m x_i
  $$

- 方差
  $$
  \sigma_B^2 = \frac{1}{m} \sum_{i=1}^m (x_i - \mu_B)^2
  $$











### 1.2 标准化

使用计算得到的均值和方差对数据进行标准化，使得每个特征的均值为0，方差为1。

- **标准化后的值**
  $$
  \hat{x}_i = \frac{x_i - \mu_B}{\sqrt{\sigma_B^2 + \epsilon}}
  $$
  其中，$$\epsilon$$ 是一个很小的常数，防止除以零的情况。

### **1.3 缩放和平移**

**目的**：
标准化后的数据会丢失原始分布的特征信息（如均值和方差）。通过可学习的缩放参数 $γ​$ 和平移参数 $β​$，网络能**自适应地恢复或调整分布**，保留非线性表达能力。

**数学表达**：

$$y_i=γ⋅\hat{x}_i +β$$

其中：

- $\hat{x}_i $ 是标准化后的数据
- $γ$（缩放）：**恢复或调整特征方差**
- $β$（平移）：**恢复或调整特征均值**

**参数特性**：

- $γ$ 和 $β$ 是**可训练参数**，它们会随着网络的训练过程通过反向传播进行更新。
- 网络通过 $γ$ 和 $β$ 自主决定标准化强度：
  - 若 $γ=\sqrt{\sigma_B^2 + \epsilon}$ 且 $β=μ_B$ → **完全抵消标准化**（输出 $y_i=x_i$）。
  - 若 $γ=1,β=0$ → **完全服从标准化分布**（输出 $$y_i=\hat{x}_i $$）。

### 1.4 更新全局统计量

通过指数移动平均（Exponential Moving Average, EMA）更新**全局均值和方差(提供给测试数据进行标准化)**：
$$
μ_{global}=(1−momentum)⋅μ_{global}+momentum⋅μ_B\\

σ_{global}^2=(1−momentum)⋅σ_{global}^2+momentum⋅σ_B^2
$$
其中，momentum 是一个超参数，控制当前 mini-batch 统计量对全局统计量的贡献。

momentum 是一个介于 0 和 1 之间的值，控制当前 mini-batch 统计量的权重。PyTorch 中 momentum 的默认值是 0.1。

**与优化器中的 momentum 的区别**

- 批量标准化中的 momentum：
  - 用于更新全局统计量（均值和方差）。
  - 控制当前 mini-batch 统计量对全局统计量的贡献。
- 优化器中的 momentum：
  - 用于加速梯度下降过程，帮助跳出局部最优。
  - 例如，SGD 优化器中的 momentum 参数。

两者虽然名字相同，但作用完全不同，不要混淆。

## 2. 测试阶段的批量标准化

在测试阶段，由于没有 mini-batch 数据，无法直接计算当前 batch 的均值和方差。因此，使用训练阶段通过 EMA 计算的全局统计量（均值和方差）来进行标准化。

在测试阶段，使用全局统计量对输入数据进行标准化：
$$
\hat x_i=\frac{x_i−μ_{global}}{\sqrt{σ_{global}^2+ϵ}}
$$
然后对标准化后的数据进行缩放和平移：
$$
yi=γ⋅\hat{x}_i+β
$$
**为什么使用全局统计量？**

**一致性**：

- 在测试阶段，输入数据通常是单个样本或少量样本，无法准确计算均值和方差。
- 使用全局统计量可以确保测试阶段的行为与训练阶段一致。

**稳定性**：

- 全局统计量是通过训练阶段的大量 mini-batch 数据计算得到的，能够更好地反映数据的整体分布。
- 使用全局统计量可以减少测试阶段的随机性，使模型的输出更加稳定。

**效率**：

- 在测试阶段，使用预先计算的全局统计量可以避免重复计算，提高效率。

## 3. 作用

批量标准化（Batch Normalization, BN）通过以下几个方面来提高神经网络的训练稳定性、加速训练过程并减少过拟合：

### **3.1 解决“内部协变量偏移”**

- **问题**：随着网络训练，前一层的参数更新会改变后一层的输入分布（数据的均值和方差变化）。这迫使后续层需要不断适应新的数据分布，导致训练不稳定、收敛变慢。
- **BN 的解决方式**：
  在每一层的输入后插入一个标准化步骤，强制将该层的输入数据**调整为均值为 0、方差为 1** 的标准分布

### 3.2 缓解梯度问题

标准化处理可以防止激活值过大或过小，避免了激活函数（如 Sigmoid 或 Tanh）饱和的问题，从而缓解梯度消失或爆炸的问题。

### 3.3 加速训练

由于 BN 使得每层的输入数据分布更为稳定，因此模型可以使用更高的学习率进行训练。这可以加快收敛速度，并减少训练所需的时间。

### 3.4 减少过拟合

- **类似于正则化**：虽然 BN 不是一种传统的正则化方法，但它通过对每个批次的数据进行标准化，可以起到一定的正则化作用。它通过在训练过程中引入了噪声（由于批量均值和方差的估计不完全准确），这有助于提高模型的泛化能力。
- **避免对单一数据点的过度拟合**：BN 强制模型在每个批次上进行标准化处理，减少了模型对单个训练样本的依赖。这有助于模型更好地学习到数据的整体特征，而不是对特定样本的噪声进行过度拟合。

## 4.函数说明

`torch.nn.BatchNorm1d` 是 PyTorch 中用于一维数据的批量标准化（Batch Normalization）模块。

```python
torch.nn.BatchNorm1d(
    num_features,         # 输入数据的特征维度
    eps=1e-05,           # 用于数值稳定性的小常数
    momentum=0.1,        # 用于计算全局统计量的动量
    affine=True,         # 是否启用可学习的缩放和平移参数
    track_running_stats=True,  # 是否跟踪全局统计量
    device=None,         # 设备类型（如 CPU 或 GPU）
    dtype=None           # 数据类型
)
```

参数说明：

eps：用于数值稳定性的小常数，添加到方差的分母中，防止除零错误。默认值：1e-05

momentum：用于计算全局统计量（均值和方差）的动量。默认值：0.1，参考本节1.4

affine：是否启用可学习的缩放和平移参数（γ和 β）。如果 affine=True，则模块会学习两个参数；如果 affine=False，则不学习参数，直接输出标准化后的值 $\hat x_i$。默认值：True

track_running_stats：是否跟踪全局统计量（均值和方差）。如果 track_running_stats=True，则在训练过程中计算并更新全局统计量，并在测试阶段使用这些统计量。如果 track_running_stats=False，则不跟踪全局统计量，每次标准化都使用当前 mini-batch 的统计量。默认值：True

## 4. 代码实现

```python
import torch
from torch import nn
from matplotlib import pyplot as plt

from sklearn.datasets import make_circles
from sklearn.model_selection import train_test_split
from torch.nn import functional as F
from torch import optim

# 数据准备
# 生成非线性可分数据（同心圆）
# n_samples	int	总样本数（默认100），内外圆各占一半
# noise	float	添加到数据中的高斯噪声标准差（默认0.0）
# factor	float	内圆与外圆的半径比（默认0.8）
# random_state	int	随机数种子，保证可重复性

# 输出数据
# X: 二维坐标数组，形状 (n_samples, 2)
# 每行是一个数据点的 [x, y] 坐标
# y: 类别标签 0（外圆）或 1（内圆），形状 (n_samples,)
x, y = make_circles(n_samples=2000, noise=0.1, factor=0.4, random_state=42)
x = torch.tensor(x, dtype=torch.float)
y = torch.tensor(y, dtype=torch.long)

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42)

# 可视化原始训练数据和测试数据
plt.scatter(x[:, 0], x[:, 1], c=y, cmap='coolwarm', edgecolors='k')
plt.show()


# 定义BN模型
class NetWithBN(nn.Module):
    def __init__(self):
        super().__init__()
        self.fc1 = nn.Linear(2, 64)
        self.bn1 = nn.BatchNorm1d(64)
        self.fc2 = nn.Linear(64, 32)
        self.bn2 = nn.BatchNorm1d(32)
        self.fc3 = nn.Linear(32, 2)

    def forward(self, x):
        x = F.relu(self.bn1(self.fc1(x)))
        x = F.relu(self.bn2(self.fc2(x)))
        x = self.fc3(x)
        return x


# 定义无BN模型
class NetWithoutBN(nn.Module):
    def __init__(self):
        super().__init__()
        self.fc1 = nn.Linear(2, 64)
        self.fc2 = nn.Linear(64, 32)
        self.fc3 = nn.Linear(32, 2)

    def forward(self, x):
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = self.fc3(x)
        return x


# 定义训练函数
def train(model, x_train, y_train, x_test, y_test, name, lr=0.1, epochs=500):
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.SGD(model.parameters(), lr=lr)

    train_loss = []
    test_acc = []

    for epoch in range(epochs):
        model.train()
        y_pred = model(x_train)
        loss = criterion(y_pred, y_train)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        train_loss.append(loss.item())

        model.eval()
        with torch.no_grad():
            y_test_pred = model(x_test)
            _, pred = torch.max(y_test_pred, dim=1)
            correct = (pred == y_test).sum().item()
            test_acc.append(correct / len(y_test))

        if epoch % 100 == 0:
            print(f'{name}|Epoch:{epoch},loss:{loss.item():.4f},acc:{test_acc[-1]:.4f}')
    return train_loss, test_acc


model_bn = NetWithBN()
model_nobn = NetWithoutBN()

bn_train_loss, bn_test_acc = train(model_bn, x_train, y_train, x_test, y_test, name='BN')
nobn_train_loss, nobn_test_acc = train(model_nobn, x_train, y_train, x_test, y_test, name='NoBN')


def plot(bn_train_loss, nobn_train_loss, bn_test_acc, nobn_test_acc):
    fig = plt.figure(figsize=(12, 5))
    ax1 = fig.add_subplot(1, 2, 1)
    ax1.plot(bn_train_loss, 'b', label='BN')
    ax1.plot(nobn_train_loss, 'r', label='NoBN')
    ax1.legend()

    ax2 = fig.add_subplot(1, 2, 2)
    ax2.plot(bn_test_acc, 'b', label='BN')
    ax2.plot(nobn_test_acc, 'r', label='NoBN')
    ax2.legend()
    plt.show()


plot(bn_train_loss, nobn_train_loss, bn_test_acc, nobn_test_acc)

```

# 