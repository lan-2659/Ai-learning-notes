# 参数初始化

神经网络的参数初始化是训练深度学习模型的关键步骤之一。初始化参数（通常是权重和偏置）会对模型的训练速度、收敛性以及最终的性能产生重要影响。下面是关于神经网络参数初始化的一些常见方法及其相关知识点。

官方文档参考：https://pytorch.org/docs/stable/nn.init.html

## 1. 固定值初始化

固定值初始化是指在神经网络训练开始时，将所有权重或偏置初始化为一个特定的常数值。这种初始化方法虽然简单，但在实际深度学习应用中通常并不推荐。因为会导致**对称性问题** 。

**对称性问题**

- 现象：同一层的所有神经元具有完全相同的初始权重和偏置。
- 后果：
  - 在反向传播时，所有神经元会收到相同的梯度，导致权重更新完全一致。无论训练多久，同一层的神经元本质上会保持相同的功能（相当于“一个神经元”的多个副本），极大降低模型的表达能力。
  - 所有输入特征被同等对待，无法学习特征间的不同重要性

### 1.1 全零初始化

将神经网络中的所有权重参数初始化为0。

**方法**：将所有权重初始化为零。

**缺点**：导致对称性破坏，每个神经元在每一层中都会执行相同的计算，模型无法学习。

**应用场景**：通常不用来初始化权重，但可以用来初始化偏置。

代码演示：

```python
import torch
import torch.nn as nn

def test004():
    # 3. 全0参数初始化
    linear = nn.Linear(in_features=6, out_features=4)
    # 初始化权重参数
    nn.init.zeros_(linear.weight)
    # 打印权重参数
    print(linear.weight)


if __name__ == "__main__":
    test004()

```

打印结果：

```python
Parameter containing:
tensor([[0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0.]], requires_grad=True)
```

### 1.2 全1初始化

全1初始化会导致网络中每个神经元接收到相同的输入信号，进而输出相同的值，这就无法进行学习和收敛。所以全1初始化只是一个理论上的初始化方法，但在实际神经网络的训练中并不适用。

代码演示：

```python
import torch
import torch.nn as nn


def test003():
    # 3. 全1参数初始化
    linear = nn.Linear(in_features=6, out_features=4)
    # 初始化权重参数
    nn.init.ones_(linear.weight)
    # 打印权重参数
    print(linear.weight)


if __name__ == "__main__":
    test003()

```

输出结果：

```python
Parameter containing:
tensor([[1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1.]], requires_grad=True)
```

### 1.3 任意常数初始化

将所有参数初始化为某个非零的常数（如 0.1，-1 等）。虽然不同于全0和全1，但这种方法依然不能避免对称性破坏的问题。

```python
import torch
import torch.nn as nn


def test002():
    # 2. 固定值参数初始化
    linear = nn.Linear(in_features=6, out_features=4)
    # 初始化权重参数
    nn.init.constant_(linear.weight, 0.63)
    # 打印权重参数
    print(linear.weight)
    pass


if __name__ == "__main__":
    test002()

```

输出结果：

```python
Parameter containing:
tensor([[0.6300, 0.6300, 0.6300, 0.6300, 0.6300, 0.6300],
        [0.6300, 0.6300, 0.6300, 0.6300, 0.6300, 0.6300],
        [0.6300, 0.6300, 0.6300, 0.6300, 0.6300, 0.6300],
        [0.6300, 0.6300, 0.6300, 0.6300, 0.6300, 0.6300]], requires_grad=True)
```

参考2：

```python
import torch
import torch.nn as nn


def test002():
    net = nn.Linear(2, 2, bias=True)
    # 假设一个数值
    x = torch.tensor([[0.1, 0.95]])

    # 初始化权重参数
    net.weight.data = torch.tensor([
        [0.1, 0.2], 
        [0.3, 0.4]
    ])

    # 输出什么：权重参数会转置
    output = net(x)
    print(output, net.bias)
    pass


if __name__ == "__main__":
    test002()

```

## 2. 随机初始化

**方法**：将权重初始化为随机的小值，通常从正态分布或均匀分布中采样。

**优点**

1. **打破对称性（核心优势）**
2. **简单容易实现**

**缺点**

1. **初始化范围难以确定**
   - 值过小：可能导致梯度消失（信号逐层衰减）
   - 值过大：可能导致梯度爆炸（数值不稳定）
2. **训练结果不稳定**
   - 不同的随机种子可能导致显著不同的训练结果
3. **不适用于特定激活函数**
   - 对于ReLU等激活函数，全正/负初始化可能导致"神经元死亡"
   - 对sigmoid/tanh等饱和激活函数，大值初始化可能使梯度消失

**应用场景**：这是最基本的初始化方法，通过随机初始化避免对称性破坏。

代码演示：均匀分布初始化

```python
import torch
import torch.nn as nn


def test001():
    # 1. 均匀分布随机初始化
    linear = nn.Linear(in_features=6, out_features=4)
    # 初始化权重参数
    nn.init.uniform_(linear.weight)
    # 打印权重参数
    print(linear.weight)


if __name__ == "__main__":
    test001()

```

打印结果：

```python
Parameter containing:
tensor([[0.4080, 0.7444, 0.7616, 0.0565, 0.2589, 0.0562],
        [0.1485, 0.9544, 0.3323, 0.9802, 0.1847, 0.6254],
        [0.6256, 0.2047, 0.5049, 0.3547, 0.9279, 0.8045],
        [0.1994, 0.7670, 0.8306, 0.1364, 0.4395, 0.0412]], requires_grad=True)
```

代码演示：正态分布初始化

```python
import torch
import torch.nn as nn


def test005():
    # 5. 正太分布初始化
    linear = nn.Linear(in_features=6, out_features=4)
    # 初始化权重参数
    nn.init.normal_(linear.weight, mean=0, std=1)
    # 打印权重参数
    print(linear.weight)


if __name__ == "__main__":
    test005()

```

打印结果：

```python
Parameter containing:
tensor([[ 1.5321,  0.2394,  0.0622,  0.4482,  0.0757, -0.6056],
        [ 1.0632,  1.8069,  1.1189,  0.2448,  0.8095, -0.3486],
        [-0.8975,  1.8253, -0.9931,  0.7488,  0.2736, -1.3892],
        [-0.3752,  0.0500, -0.1723, -0.4370, -1.5334, -0.5393]],
       requires_grad=True)
```

## 3. Xavier 初始化

**Xavier 初始化**（由 Xavier Glorot 在 2010 年提出）是一种**自适应权重初始化方法**，专门为解决神经网络训练初期的梯度消失或爆炸问题而设计。Xavier 初始化也叫做Glorot初始化。**Xavier 初始化的核心思想是根据输入和输出的维度来初始化权重**，**保持网络中每一层输入(前一层激活后的输出或原始输入)的方差和输出(即 y=Wx+b,还没被激活函数处理前的值)的方差在前向传播过程中大致相同**。具体来说，权重的初始化范围取决于前一层的神经元数量（输入维度）和当前层的神经元数量（输出维度）。

**方法**：根据输入和输出神经元的数量来选择权重的初始值。 

数学原理：

**(1) 前向传播的方差一致性**

**对于当前层的任意一个神经元，假设输入所有 x 的均值都为 0，方差都为 $Var(x)$，所有权重 W的均值都为 0，方差都为 $Var(w)$**，则输出 $z=Wx$的方差为：
$$
Var(z)=n_{in}⋅Var(W)⋅Var(x)
$$
根据方差的定义与期望的性质，推导如下：
$$
\begin{align*}
\text{Var}(w_i) &= \mathbb{E}[w_i^2] - (\mathbb{E}[w_i])^2 = \mathbb{E}[w_i^2] - 0^2 = \mathbb{E}[w_i^2] \implies \mathbb{E}[w_i^2] = \text{Var}(w_i) \\
\end{align*}
$$

$$
\begin{align*}
\text{Var}(x_i) &= \mathbb{E}[x_i^2] - (\mathbb{E}[x_i])^2 \implies \mathbb{E}[x_i^2] = \text{Var}(x_i) + (\mathbb{E}[x_i])^2 \\
\end{align*}
$$

对于 $s_i = w_i \cdot x_i$ 的方差：
$$
\begin{align*}
\text{Var}(s_i) &= \text{Var}(w_i \cdot x_i) \\
&= \mathbb{E}[(w_i \cdot x_i)^2] - [\mathbb{E}(w_i \cdot x_i)]^2 \quad \text{// 方差定义} \\
&= \mathbb{E}[w_i^2 \cdot x_i^2] - [\mathbb{E}(w_i) \cdot \mathbb{E}(x_i)]^2 \quad \text{// 利用独立性：}\mathbb{E}[AB] = \mathbb{E}[A]\mathbb{E}[B] \\
&= \mathbb{E}[w_i^2] \cdot \mathbb{E}[x_i^2] - [0 \cdot \mathbb{E}(x_i)]^2 \quad \text{// 因}\mathbb{E}[w_i] = 0 \\
&= \mathbb{E}[w_i^2] \cdot \mathbb{E}[x_i^2] \\
&= \text{Var}(w_i) \cdot [\text{Var}(x_i) + (\mathbb{E}[x_i])^2] \\
\end{align*}
$$
若输入的均值为 0（即 $\mathbb{E}[x_i] = 0$），则：
$$
\begin{align*}
\text{Var}(s_i) &= \text{Var}(w_i) \cdot \text{Var}(x_i) \\
&= \text{Var}(w) \cdot \text{Var}(x)	\text{//假设权重方差相同，输入方差相同}
\end{align*}
$$
对于总和 $z = s_1 + s_2 + \dots + s_{n_{\text{in}}}$（假设所有 $s_i$ 相互独立），总方差为各变量方差之和：
$$
\begin{align*}
\text{Var}(z) &= \text{Var}(s_1) + \text{Var}(s_2) + \dots + \text{Var}(s_{n_{\text{in}}}) \\
&= [\text{Var}(w) \cdot \text{Var}(x)] + [\text{Var}(w) \cdot \text{Var}(x)] + \dots + [\text{Var}(w) \cdot \text{Var}(x)] \\
&= n_{\text{in}} \cdot [\text{Var}(w) \cdot \text{Var}(x)] \\
\end{align*}
$$
为了使 Var(z)=Var(x)，需要：
$$
n_{in}⋅Var(W)=1  ⟹  Var(W)=\frac{1}{n_{in}}
$$
其中 $n_{in}$是输入维度（fan_in）。

**(2) 反向传播的梯度方差一致性**

在反向传播过程中，梯度 $\frac{∂L}{∂x}$ 是通过链式法则计算得到的，其中 *L* 是损失函数，*x* 是输入，*z* 是输出。梯度$\frac{∂L}{∂x}$可以表示为： 
$$
\frac{∂L}{∂x}=\frac{∂L}{∂z}.\frac{∂z}{∂x}
$$
假设 $z=W^Tx$，其中 $W$ 是权重矩阵，那么 $\frac{∂z}{∂x}=W^T$。因此，梯度  $\frac{∂L}{∂x}$可以写为： $\frac{∂L}{∂x}=\frac{∂L}{∂z}W^T$

反向传播时梯度 $\frac{∂L}{∂x}$ 的方差应与 $\frac{∂L}{∂z}$ 相同，因此：
$$
n_{out}⋅Var(W)=1  ⟹  Var(W)=\frac{1}{n_{out}}
$$
其中 $n_{out}$是输出维度（fan_out）。为了保持梯度的方差一致性，我们需要确保每个输入维度 $n_{in}$的梯度方差与输出维度 $n_{out}$的梯度方差相同。因此，我们需要将 *W* 的方差乘以 $n_{out}$，以确保梯度的方差在反向传播过程中保持一致。

**(3) 综合考虑**

为了同时平衡前向传播和反向传播，Xavier 采用：
$$
Var(W)=\frac{2}{n_{in}+n_{out}}
$$
权重从以下分布中采样：

**均匀分布**：
$$
W\sim\mathrm{U}\left(-\frac{\sqrt{6}}{\sqrt{n_\mathrm{in}+n_\mathrm{out}}},\frac{\sqrt{6}}{\sqrt{n_\mathrm{in}+n_\mathrm{out}}}\right)
$$
在Xavier初始化中，我们选择 $a=−\sqrt{\frac{6}{n_{in}+n_{out}}}$ 和 $b=\sqrt{\frac{6}{n_{in}+n_{out}}}$，这样方差为： 
$$
Var(W)=\frac{(b−a)^2}{12}=\frac{(2\sqrt{\frac{6}{n_{in}+n_{out}}})^2}{12}=\frac{4⋅\frac{6}{nin+nout}}{12}=\frac{2}{n_{in}+n_{out}}
$$
**正态分布**：
$$
W\sim\mathrm{N}\left(0,\frac{2}{n_\mathrm{in}+n_\mathrm{out}}\right)
$$

$$
\mathcal{N}(0, \text{std}^2)
$$

其中 $$n_{\text{in}}$$ 是当前层的输入神经元数量，$$n_{\text{out}}$$是输出神经元数量。

在前向传播中，输出的方差受 $n_{in}$ 影响。在反向传播中，梯度的方差受 $n_{out}$ 影响。

**优点**：平衡了输入和输出的方差，适合$$Sigmoid$$ 和 $$Tanh$$ 激活函数。

**应用场景**：常用于浅层网络或使用$$Sigmoid$$ 、$$Tanh$$ 激活函数的网络。

代码演示：

```python
import torch
import torch.nn as nn


def test007():
    # Xavier初始化：正态分布
    linear = nn.Linear(in_features=6, out_features=4)
    nn.init.xavier_normal_(linear.weight)
    print(linear.weight)

    # Xavier初始化：均匀分布
    linear = nn.Linear(in_features=6, out_features=4)
    nn.init.xavier_uniform_(linear.weight)
    print(linear.weight)


if __name__ == "__main__":
    test007()

```

打印结果：

```python
Parameter containing:
tensor([[-0.4838,  0.4121, -0.3171, -0.2214, -0.8666, -0.4340],
        [ 0.1059,  0.6740, -0.1025, -0.1006,  0.5757, -0.1117],
        [ 0.7467, -0.0554, -0.5593, -0.1513, -0.5867, -0.1564],
        [-0.1058,  0.5266,  0.0243, -0.5646, -0.4982, -0.1844]],
       requires_grad=True)
Parameter containing:
tensor([[-0.5263,  0.3455,  0.6449,  0.2807, -0.3698, -0.6890],
        [ 0.1578, -0.3161, -0.1910, -0.4318, -0.5760,  0.3746],
        [ 0.2017, -0.6320, -0.4060,  0.3903,  0.3103, -0.5881],
        [ 0.6212,  0.3077,  0.0783, -0.6187,  0.3109, -0.6060]],
       requires_grad=True)
```

## 4. He初始化

也叫kaiming 初始化（pytorch中的默认初始化方式）。He 初始化的核心思想是**调整权重的初始化范围，使得每一层的输出的方差保持一致**。与 Xavier 初始化不同，He 初始化专门针对 ReLU 激活函数的特性进行了优化。

**数学推导**

**(1) 前向传播的方差一致性**

对于 ReLU 激活函数，输出的方差为：
$$
Var(z)=\frac{1}{2}n_{in}⋅Var(W)⋅Var(x)
$$
（因为 ReLU 使一半神经元输出为 0，方差减半）
为使 Var(z)=Var(x)，需：
$$
\frac{1}{2}n_{in}⋅Var(W)=1  ⟹  Var(W)=\frac{2}{n_{in}}
$$
**(2) 反向传播的梯度一致性**

类似地，反向传播时梯度方差需满足：
$$
Var(\frac{∂L}{∂x})=\frac{1}{2}n_{out}⋅Var(W)⋅Var(\frac{∂L}{∂z})
$$
因此：
$$
Var(W)=\frac{2}{n_{out}}
$$
**(3) 两种模式**

- **`fan_in` 模式**（默认）：优先保证前向传播稳定，方差 $\frac{2}{n_{in}}$。
- **`fan_out` 模式**：优先保证反向传播稳定，方差$\frac{2}{n_{out}}$。

**方法**：专门为 ReLU 激活函数设计。权重从以下分布中采样：

均匀分布：
$$
W\sim\mathrm{U}\left(-\frac{\sqrt{6}}{\sqrt{n_\mathrm{in}}},\frac{\sqrt{6}}{\sqrt{n_\mathrm{in}}}\right)
$$
正态分布：
$$
W\sim\mathrm{N}\left(0,\frac{2}{n_\mathrm{in}}\right)
$$
其中 $$n_{\text{in}}$$ 是当前层的输入神经元数量。

**优点**：适用于$$ReLU$$ 和 $$Leaky ReLU$$ 激活函数。

**应用场景**：深度网络，尤其是使用 ReLU 激活函数时。

代码演示：

```python
import torch
import torch.nn as nn


def test006():
    # He初始化：正态分布
    linear = nn.Linear(in_features=6, out_features=4)
    nn.init.kaiming_normal_(linear.weight, nonlinearity="relu", mode='fan_in')
    print(linear.weight)

    # He初始化：均匀分布
    linear = nn.Linear(in_features=6, out_features=4)
    nn.init.kaiming_uniform_(linear.weight, nonlinearity="relu", mode='fan_out')
    print(linear.weight)


if __name__ == "__main__":
    test006()

```

输出结果：

```python
Parameter containing:
tensor([[ 1.4020,  0.2030,  0.3585, -0.7419,  0.6077,  0.0178],
        [-0.2860, -1.2135,  0.0773, -0.3750, -0.5725,  0.9756],
        [ 0.2938, -0.6159, -1.1721,  0.2093,  0.4212,  0.9079],
        [ 0.2050,  0.3866, -0.3129, -0.3009, -0.6659, -0.2261]],
       requires_grad=True)

Parameter containing:
tensor([[-0.1924, -0.6155, -0.7438, -0.2796, -0.1671, -0.2979],
        [ 0.7609,  0.9836, -0.0961,  0.7139, -0.8044, -0.3827],
        [ 0.1416,  0.6636,  0.9539,  0.4735, -0.2384, -0.1330],
        [ 0.7254, -0.4056, -0.7621, -0.6139, -0.6093, -0.2577]],
       requires_grad=True)
```

## 5. 总结

在使用Torch构建网络模型时，每个网络层的参数都有默认的初始化方法，同时还可以通过以上方法来对网络参数进行初始化。

# 