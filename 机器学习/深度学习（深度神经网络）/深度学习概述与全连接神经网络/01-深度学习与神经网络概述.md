---
typora-root-url: ..
---

# 一、深度学习概述



## 1. 什么是深度学习

​	人工智能、机器学习和深度学习之间的关系：

<img src="media/03e865d2244380ba5234aa92a4d6bfee.png" alt="img" style="zoom: 67%;" />



​	机器学习是实现人工智能的一种途径，深度学习是机器学习的子集，区别如下：

<img src="media/image-20240826100014295.png" alt="image-20240826100014295" style="zoom:38%;" />



​	传统机器学习算法依赖人工设计特征、提取特征，而深度学习依赖算法自动提取特征。深度学习模仿人类大脑的运行方式，从大量数据中学习特征，这也是深度学习被看做黑盒子、可解释性差的原因。

​	随着算力的提升，深度学习可以处理图像，文本，音频，视频等各种内容，主要应用领域有：

1. 图像处理：分类、目标检测、图像分割（语义分割）
2. 自然语言处理：LLM、NLP、Transformer
3. 语音识别：对话机器人、智能客服（语音+NLP）
4. 自动驾驶：语义分割（行人、车辆、实线等）
5. LLM：大Large语言Language模型Model
6. 机器人：非常火的行业

有了大模型的加持，AI+各行各业。



## 2. 深度学习发展历史



<img src="./media/image-20240521100225425.png" style="zoom: 80%;" />



​	   深度学习其实并不是新的事物，深度学习所需要的神经网络技术起源于20世纪50年代，叫做感知机。当时使用单层感知机，因为只能学习线性可分函数，连简单的异或(XOR)等线性不可分问题都无能为力，1969年Marvin Minsky写了一本叫做《Perceptrons》的书，他提出了著名的两个观点：1.单层感知机没用，我们需要多层感知机来解决复杂问题 2.没有有效的训练算法。

​	   20世纪80年代末期，用于人工神经网络的反向传播算法（也叫Back Propagation算法或者BP算法）的发明，给机器学习带来了希望，掀起了基于统计模型的机器学习热潮。这个热潮一直持续到今天。人们发现，利用BP算法可以让一个人工神经网络模型从大量训练样本中学习统计规律，从而对未知事件做预测。这种基于统计的机器学习方法比起过去基于人工规则的系统，在很多方面显出优越性。这个时候的人工神经网络，虽也被称作多层感知机（Multi-layer Perceptron），但实际是种只含有一层隐层节点的浅层模型。

​		2006年，杰弗里·辛顿以及他的学生鲁斯兰·萨拉赫丁诺夫正式提出了深度学习的概念。

​		2012年，在著名的ImageNet图像识别大赛中，杰弗里·辛顿领导的小组采用深度学习模型AlexNet一举夺冠。AlexNet采用ReLU激活函数，从根本上解决了梯度消失问题，并采用GPU极大的提高了模型的运算速度。

​		同年，吴恩达教授和Jeff Dean主导的深度神经网络DNN技术在ImageNet评测中把错误率从26％降低到15％，再一次吸引了学术界和工业界对于深度学习领域的关注。

​	2016年，随着谷歌公司基于深度学习开发的AlphaGo以4:1的比分战胜了国际顶尖围棋高手李世石，深度学习的热度一时无两。后来，AlphaGo又接连和众多世界级围棋高手过招，均取得了完胜。这也证明了在围棋界，基于深度学习技术的机器人已经超越了人类。

​	2017年，基于强化学习算法的AlphaGo升级版AlphaGo Zero横空出世。其采用“从零开始”、“无师自通”的学习模式，以100:0的比分轻而易举打败了之前的AlphaGo。除了围棋，它还精通国际象棋等其它棋类游戏，可以说是真正的棋类“天才”。此外在这一年，深度学习的相关算法在医疗、金融、艺术、无人驾驶等多个领域均取得了显著的成果。所以，也有专家把2017年看作是深度学习甚至是人工智能发展最为突飞猛进的一年。

​	2019年，基于Transformer 的自然语言模型的持续增长和扩散，这是一种语言建模神经网络模型，可以在几乎所有任务上提高NLP的质量。Google甚至将其用作相关性的主要信号之一，这是多年来最重要的更新。

​	2020年，深度学习扩展到更多的应用场景，比如积水识别，路面塌陷等，而且疫情期间，在智能外呼系统，人群测温系统，口罩人脸识别等都有深度学习的应用。



## 3. 深度学习的优势



<img src="./media/image-20240521100606369.png" style="zoom:67%;" />





# 二、神经网络

​		我们要学习的**深度学习**(Deep Learning)是神经网络的一个子领域，主要关注更深层次的神经网络结构，也就是**深层神经网络**（Deep Neural Networks，DNNs）。所以，我们需要先搞清楚什么是神经网络！

## 1. 感知神经网络

神经网络（Neural Networks）是一种模拟人脑神经元网络结构的计算模型，用于处理复杂的模式识别、分类和预测等任务。生物神经元如下图：

<img src="media/image-20240826104526463.png" style="zoom:38%;" />



**生物学：**

人脑可以看做是一个生物神经网络，由众多的神经元连接而成

- 树突：从其他神经元接收信息的分支
- 细胞核：处理从树突接收到的信息
- 轴突：被神经元用来传递信息的生物电缆
- 突触：轴突和其他神经元树突之间的连接

人脑神经元处理信息的过程：

- 多个信号到达树突，然后整合到细胞体的细胞核中
- 当积累的信号超过某个阈值，细胞就会被激活
- 产生一个输出信号，由轴突传递。

神经网络由多个互相连接的节点（即人工神经元）组成。

## **2. 人工神经元**

​		人工神经元(Artificial Neuron)是神经网络的基本构建单元，模仿了生物神经元的工作原理。其核心功能是接收输入信号，经过加权求和和非线性激活函数处理后，输出结果。

### 2.1 构建人工神经元

人工神经元接受多个输入信息，对它们进行加权求和，再经过激活函数处理，最后将这个结果输出。

<img src="media/08.png" alt="img" style="zoom:100%;" />

### 2.2 组成部分

- **输入**（Inputs）: 代表输入数据，通常用向量表示，每个输入值对应一个权重。
- **权重**（Weights）: 每个输入数据都有一个权重，表示该输入对最终结果的重要性。
- **偏置**（Bias）: 一个额外的可调参数，作用类似于线性方程中的截距，帮助调整模型的输出。
- **加权求和:** 神经元将输入乘以对应的权重后求和，再加上偏置。
- **激活函数**（Activation Function）: 用于将加权求和后的结果转换为输出结果，引入非线性特性，使神经网络能够处理复杂的任务。常见的激活函数有Sigmoid、ReLU（Rectified Linear Unit）、Tanh等。

### 2.3 数学表示

如果有 n 个输入 $$x_1, x_2, \ldots, x_n$$，权重分别为 $$w_1, w_2, \ldots, w_n$$，偏置为 $$b$$，则神经元的输出 $$y$$ 表示为：
$$
z=\sum_{i=1}^nw_i\cdot x_i+b \\
y=\sigma(z)
$$
其中，$$\sigma(z)$$ 是激活函数。

例如：

线性回归：
$$
y=\sum_{i=1}^nw_i\cdot x_i+b \\
$$
线性回归不需要激活函数

逻辑回归：
$$
z=\sum_{i=1}^nw_i\cdot x_i+b \\
y=\sigma(z)=sigmoid(z)=\frac{1}{1+e^{-z}}
$$

### 2.4 对比生物神经元

人工神经元和生物神经元对比如下表：

| 生物神经元 | 人工神经元            |
| ----- | ---------------- |
| 细胞核   | 节点 (加权求和 + 激活函数) |
| 树突    | 输入               |
| 轴突    | 带权重的连接           |
| 突触    | 输出               |

## 3. 深入神经网络

神经网络是由大量人工神经元按层次结构连接而成的计算模型。每一层神经元的输出作为下一层的输入，最终得到网络的输出。

### 3.1 基本结构

神经网络有下面三个基础层（Layer）构建而成：

- **输入层（Input）**: 神经网络的第一层，负责接收外部数据，不进行计算。
- **隐藏层（Hidden）**: 位于输入层和输出层之间，进行特征提取和转换。隐藏层一般有多层，每一层有多个神经元。
- - **隐藏层只有一层：单层神经网络（Single-Layer Neural Network）**
  - **隐藏层有多层（≥2层）：深度神经网络（Deep Neural Network, DNN）**
- **输出层（Output）**: 网络的最后一层，产生最终的预测结果或分类结果

### 3.2 网络构建

我们使用多个神经元来构建神经网络，相邻层之间的神经元相互连接，并给每一个连接分配一个权重，经典如下：

<img src="media/image-20240826115241535.png" alt="image-20240826115241535" style="zoom:37%;" />

**注意：同一层的各个神经元之间是没有连接的。**

### 3.3 全连接神经网络

前馈神经网络（Feedforward Neural Network，FNN）是一种最基本的神经网络结构，其特点是信息从输入层经过隐藏层单向传递到输出层，**没有反馈或循环连接**。

全连接神经网络（Fully Connected Neural Network，FCNN）是前馈神经网络的一种，每一层的神经元与上一层的所有神经元全连接，常用于图像分类、文本分类等任务。

<img src="media/QQ20250225-110403.png" style="zoom:37%;" />

<img src="media/QQ20250225-111854.png" style="zoom:37%;" />

如上图，网络中每个神经元：
$$
z_1 = x_1*w_1 + x_2*w_2+b_1 \\
z_2 = x_1*w_1 + x_2*w_2+b_2 \\
z_3 = x_1*w_1 + x_2*w_2+b_3
$$
说明：三个等式中的w1和w2在这里只是为了方便表示对应x1和x2的权重，实际三个等式中的w值是不同的。

向量x为：$[x_1,x_2]$

向量w：$$\begin{pmatrix}w_1,w_2\\w_1,w_2\\w_1,w_2 \end{pmatrix}$$，其形状为（3，2），3是神经元节点个数，2是向量x的个数

向量z：$$[z_1,z_2,z_3]$$

向量b：$[b_1,b_2,b_3]$

所以用向量表示为：
$$
z = \begin{pmatrix}z_1,z_2,z_3 \end{pmatrix}=\begin{pmatrix}x_1,x_2 \end{pmatrix}\begin{pmatrix}w_1,w_1,w_1\\w_2,w_2,w_2\end{pmatrix}+\begin{pmatrix}b_1,b_2,b_3 \end{pmatrix}=\begin{pmatrix}x_1,x_2 \end{pmatrix}\begin{pmatrix}w_1,w_2\\w_1,w_2\\w_1,w_2 \end{pmatrix}^T + \begin{pmatrix}b_1,b_2,b_3 \end{pmatrix}=xw^T+b
$$

- x是输入数据，形状为 (batch_size, in_features)。
- W是权重矩阵，形状为 (out_features, in_features)。
- b是偏置项，形状为 (out_features,)。
- z是输出数据，形状为 (batch_size, out_features)。

#### 3.3.1 特点

- 全连接层: 层与层之间的每个神经元都与前一层的所有神经元相连。
- 权重数量: 由于全连接的特点，权重数量较大，容易导致计算量大、模型复杂度高。
- 学习能力: 能够学习输入数据的全局特征，但对于高维数据却不擅长捕捉局部特征（如图像就需要CNN）。

#### 3.3.2 计算步骤

1. 数据传递: 输入数据经过每一层的计算，逐层传递到输出层。
2. 激活函数: 每一层的输出通过激活函数处理。
3. 损失计算: 在输出层计算预测值与真实值之间的差距，即损失函数值。
4. 反向传播（Back Propagation）: 通过反向传播算法计算损失函数对每个权重的梯度，并更新权重以最小化损失。