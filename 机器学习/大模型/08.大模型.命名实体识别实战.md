

# 一、NER基础认知

Named Entity Recognition, **NER**。

命名实体识别是NLP中的一项核心任务，旨在从非结构化的文本中识别并分类特定的实体（如人名、地名、组织机构等），并将其归类到预定义的类别中。

## 1. 任务定义

- 目标：从文本中检测出命名实体，也就是具有特定意义的专有名词，并为其分配预定义的类别标签。
- 输入：原始文本，如：$苹果公司于1976年在美国加州成立$。
- 输出：带有实体类别标注的文本，如：$[苹果公司]ORG于1976年[DATE]在[美国]LOC[加州]LOC成立。$



## 2. 标记格式

在NER任务中，我们需要明确文本中实体的**类型**和**边界位置**，这是实现NER任务的基础编码规则。

### 2.1 标记作用

NER任务需要解决两个核心问题：

- 识别实体边界：==张三==是一个整体人名，还是==张==和==三==分开
- 分类实体类型：是==人名==、==地名==还是==组织机构==

### 2.2 标签格式

有BIO和BIOES等格式，可以使用Label Studio、BRAT等工具。

#### 2.2.1 BIO→更常用

- B-XXX： Begin，表示某类实体的起始位置。

  如：==B-PER== → 人名的第一个字

- I-XXX： Inside，表示某类实体的中间或结尾位置。  

  如：==I-PER== → 人名的第二个及之后的字

- O：Outside，非实体部分。  

**示例**：  

- 句子：==张 三 在 北 京 工作==

- 标签：==[B-PER, I-PER, O, B-LOC, I-LOC, O]== 

  解析：==张三==（人名）、==北京==（地名）

#### 2.2.2 BIOES格式→更精细

- B-XXX：实体起始  
- I-XXX：实体中间  
- E-XXX：End，实体结尾  
- S-XXX（）：Single，单字实体  
- O：非实体  

**示例**：  

- 句子：==李 四 访 问 巴 黎==  

- 标签：==[B-PER, E-PER, O, O, B-LOC, E-LOC]==

  解析：==李四==（完整人名）、==巴黎==（完整地名）。



### 2.3 类型缩写

- PER：人名，Person 
- LOC：地名，Location  
- ORG：组织机构，Organization  
- DATE：日期  
- 可能扩展：DISEASE（疾病）、DRUG（药物）等。



### 2.4 注意事项

- 标签一致性：  

  必须统一使用BIO或BIOES，避免混用。

- 分词对齐

  - 中文需确保分词后的字/词与标签一一对应。  
  - 错误示例：==北京市==应标为 ==[B-LOC, I-LOC]==，而非单个 ==B-LOC==。

- 嵌套实体处理：  

  如==[北京大学]ORG的[北京]LOC==，需分层标注或使用特殊格式（如BIOUL）。



## 3. 模型指标

如何评价一个NER模型的效果呢？

### 3.1 精确率→Precision

- 定义：模型预测的实体中，正确的比例。  

- 公式：
  $$
  \text{Precision} = \frac{\text{预测正确的实体数（TP）}}{\text{模型预测的所有实体数（TP + FP）}}
  $$

- 意义：衡量模型的预测准确性。  

- NER特殊性：实体边界和类型必须完全匹配才算正确,部分匹配不算TP。  

### 3.2 召回率→Recall

- 定义：真实存在的实体中，模型预测出的比例。  

- 公式：
  $$
  \text{Recall} = \frac{\text{预测正确的实体数（TP）}}{\text{真实存在的所有实体数（TP + FN）}}
  $$

- 意义：衡量模型的覆盖能力。  

- NER特殊性：漏标实体或边界错误均算作FN。  

### 3.3 F1 Score

- 定义：精确率和召回率的调和平均数，综合反映模型性能。  

- 公式：
  $$
  F1 = 2 \times \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}
  $$

- 意义：平衡精确率和召回率，避免单一指标偏高导致的偏颇。  

- NER中的重要性：  

  - 黄金指标，尤其适合类别不平衡数据：非实体“O”标签占比高。  
  - 要求实体类型和边界严格匹配。  

### 3.4 准确率→Accuracy

- 定义：所有Token中预测正确的比例。  

- 公式：  
  $$
  \text{Accuracy} = \frac{\text{TP + TN}}{\text{TP + TN + FP + FN}}
  $$

- 局限性：  

  - 不推荐用于NER：因非实体==O标签==占比极高，模型全标“O”时准确率可能虚高，但 $F1$ 为 $0$。  
  - 无法反映实体识别的真实性能。  

### 3.5 Micro-F1

- 计算方式：将所有类别的 $TP、FP、FN$ 合并后计算全局 $F1$。  

- 适用场景：  

  默认选择，尤其类别不平衡时（关注整体性能）。  

### 3.6 Macro-F1

- 计算方式：对每个类别的F1单独计算后取算术平均。  
- 适用场景：  
  - 平等看待所有类别（如医疗NER中罕见病名和常见药物同等重要）。  
  - 少样本学习时避免小类别被忽略。  

### 3.7 Weighted-F1

- 计算方式：按每个类别的样本数加权平均F1。  

- 适用场景：  

  类别重要性与其样本量相关时（如数据分布不均匀但需考虑样本权重）。  



### 3.8 NER特殊性

NER对实体类型及边界都有要求。

#### 3.8.1 严格匹配

匹配规则要求：  

- TP：实体类型和边界（起始/结束位置）必须完全匹配

  真实标签 ==[B-PER, I-PER]== → 张三：

  ​	预测为 ==[B-PER, I-PER]== → TP

  ​	预测为 ==[B-PER, O]==（“张”）→ FP+FN 

- FP：模型预测的实体不存在或类型错误。  

- FN：真实实体未被预测或边界错误。  

#### 3.8.2 嵌套实体评估

- 挑战：同一文本位置可能存在多个实体（如`“[北京大学]ORG的[北京]LOC”`）。  
- 解决方法：分层评估，按实体层级分别计算F1。  



### 3.9 业务场景建议

- 高精度优先：如搜索引擎，优化Precision，减少误报。  
- 高召回优先：如医疗风险筛查，优化Recall，避免漏检。  
- 平衡场景：直接优化F1 Score，90%的NER任务默认选择。  
- 避免使用Accuracy



## 4. 环境准备

工具推荐`seqeval`或HuggingFace的`evaluate`库。

```bash
pip install evaluate seqeval
```



# 二、NER项目实战

人工智能三大模块：数据、算法、算力

官方案例：https://huggingface.co/docs/transformers/v4.50.0/en/tasks/token_classification#load-wnut-17-dataset

## 1. 数据集

使用hugginface开源数据集：https://huggingface.co/datasets/leondz/wnut_17

### 1.1 数据集分类

```css
0: O
1: B-corporation
2: I-corporation
3: B-creative-work
4: I-creative-work
5: B-group
6: I-group
7: B-location
8: I-location
9: B-person
10: I-person
11: B-product
12: I-product
```

### 1.2 数据集加载

```python
from datasets import load_dataset
import os

CURRENT_PATH = os.path.dirname(os.path.abspath(__file__))
dataset_path = os.path.join(CURRENT_PATH, "data")

if __name__ == "__main__":
    dataset = load_dataset(
        "leondz/wnut_17",
        cache_dir=dataset_path,
        trust_remote_code=True
    )
    print(dataset['train'][0])
    print(dataset["train"].features["ner_tags"].feature.names)
```

打印结果：

```css
{'id': '0', 'tokens': ['@paulwalk', 'It', "'s", 'the', 'view', 'from', 'where', 'I', "'m", 'living', 'for', 'two', 'weeks', '.', 'Empire', 'State', 'Building', '=', 'ESB', '.', 'Pretty', 'bad', 'storm', 'here', 'last', 'evening', '.'], 'ner_tags': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 8, 8, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0]}
['O', 'B-corporation', 'I-corporation', 'B-creative-work', 'I-creative-work', 'B-group', 'I-group', 'B-location', 'I-location', 'B-person', 'I-person', 'B-product', 'I-product']
```



### 1.3 数据预处理

依赖于我们的transformers完成相关工作，轻松~这里我们可以使用GPT2

#### 1.3.1 tokenizer加载

```python
import os
from transformers import AutoTokenizer

CURRENT_PATH = os.path.dirname(os.path.abspath(__file__))
model_path = os.path.join(CURRENT_PATH, "openai")

if __name__ == "__main__":
    # 加载tokenizer
    tokenizer = AutoTokenizer.from_pretrained(
        "openai-community/gpt2",
        cache_dir=model_path,
        trust_remote_code=True,
        add_prefix_space=True,  # 添加空格前缀
    )
    # 指定填充符号
    tokenizer.pad_token = tokenizer.eos_token
    # 对样本进行编码
    result = tokenizer(
        dataset["train"][0]["tokens"],
        is_split_into_words=True,  # 告诉他文本已经分割成单词
    )
    print(result)
    print(tokenizer.decode(result["input_ids"]))
```



#### 1.3.2 数据批处理

预处理为GPT模型接受的数据形式，如输入模型的是字词的编号id~

```python
    # 对每个样本进行编码的小函数
    def tokenize_function(examples):
        tokenized_inputs = tokenizer(
            examples["tokens"],
            truncation=True,
            is_split_into_words=True,
        )
        assert len(examples["ner_tags"]) == len(tokenized_inputs["input_ids"])
        tokenized_inputs["labels"] = examples["ner_tags"]
        return tokenized_inputs

    # 对数据集进行编码
    tokenized_datasets = dataset.map(
        tokenize_function,
        batched=True,
    )
    print(tokenized_datasets)
```

#### 1.3.3 数据对齐

还需要完成数据的填充，以保持一个batch是对齐的，这个在训练的时候使用：

```python
from transformers import AutoTokenizer, DataCollatorForTokenClassification
# 创建数据填充实例化对象
data_collator = DataCollatorForTokenClassification(tokenizer)
```



### 1.4 评价指标

代码参考：

```python
    # 评价指标
    import numpy as np
    import evaluate

    seqeval = evaluate.load("seqeval")
    # 获取分类
    label_list = dataset["train"].features["ner_tags"].feature.names

    def compute_metrics(p):
        predictions, labels = p
        predictions = np.argmax(
            predictions, axis=2
        )  # 维度：[batch_size, seq_len, num_labels]
        # 预测的结果
        true_predictions = [
            [label_list[p] for (p, l) in zip(prediction, label) if l != -100]
            for prediction, label in zip(predictions, labels)
        ]
        # 实际的结果
        true_labels = [
            [label_list[l] for (p, l) in zip(prediction, label) if l != -100]
            for prediction, label in zip(predictions, labels)
        ]
        # 计算指标
        results = seqeval.compute(
            predictions=true_predictions,
            references=true_labels,
        )
        return {
            "precision": results["overall_precision"],
            "recall": results["overall_recall"],
            "f1": results["overall_f1"],
            "acc": results["overall_accuracy"],
        }
```



### 1.5 模型训练

需要一些准备工作~

#### 1.5.1 标签映射

```python
    # 标签映射
    idx2tag = {idx: label_list[idx] for idx in range(len(label_list))}
    tag2idx = {label_list[idx]: idx for idx in range(len(label_list))}
    print(idx2tag)
    print(tag2idx)
```



#### 1.5.2 预训练模型

```python
    from transformers import AutoModelForTokenClassification
    model = AutoModelForTokenClassification.from_pretrained(
        "openai-community/gpt2",
        cache_dir=model_path,
        num_labels=len(label_list),
        id2label=idx2tag,
        label2id=tag2idx,
    )
```



#### 1.5.3 训练超参数

```python
    output_dir = os.path.join(CURRENT_PATH, "output")
    logging_dir = os.path.join(CURRENT_PATH, "logs")
    training_args = TrainingArguments(
        output_dir=output_dir,
        num_train_epochs=5,
        per_device_train_batch_size=16, # batch size per device during training
        per_device_eval_batch_size=16, # batch size for evaluation
        warmup_steps=500,
        learning_rate=2e-5, # initial learning rate (AdamW)
        weight_decay=0.01, # coefficient for L2 regularization
        logging_dir=logging_dir,
        logging_steps=100,
        evaluation_strategy="epoch",
        save_strategy='epoch',
        load_best_model_at_end=True
    )
```



#### 1.5.4 训练实例

```python
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=tokenized_datasets["train"],
        eval_dataset=tokenized_datasets["test"],
        compute_metrics=compute_metrics,
        tokenizer=tokenizer,
        data_collator=data_collator,
    )
```

#### 1.5.5 开始训练

```python
import warnings
from datasets import load_dataset
import os
from transformers import AutoTokenizer, DataCollatorForTokenClassification

CURRENT_PATH = os.path.dirname(os.path.abspath(__file__))
dataset_path = os.path.join(CURRENT_PATH, "data")
model_path = os.path.join(CURRENT_PATH, "nermodel")
model_name = "dslim/bert-base-NER"

if __name__ == "__main__":
    dataset = load_dataset(
        "leondz/wnut_17",
        cache_dir=dataset_path,
        trust_remote_code=True,
    )

    # 加载tokenizer
    tokenizer = AutoTokenizer.from_pretrained(
        model_name,
        cache_dir=model_path,
        trust_remote_code=True,
        # add_prefix_space=True,  # 添加前缀
    )
    # 指定填充符号
    # print(tokenizer)
    # tokenizer.pad_token = tokenizer.eos_token
    # 对样本进行编码
    result = tokenizer(
        dataset["train"][0]["tokens"],
        is_split_into_words=True,  # 告诉他文本已经分割成单词
    )

    # 对每个样本进行编码的小函数
    def tokenize_function(examples):
        tokenized_inputs = tokenizer(
            examples["tokens"],
            truncation=True,
            is_split_into_words=True,
        )
        assert len(examples["ner_tags"]) == len(tokenized_inputs["input_ids"])
        tokenized_inputs["labels"] = examples["ner_tags"]
        return tokenized_inputs

    # 对数据集进行编码
    tokenized_datasets = dataset.map(
        tokenize_function,
        batched=True,
    )

    # 创建数据填充实例化对象
    data_collator = DataCollatorForTokenClassification(tokenizer)

    # 评价指标
    import numpy as np
    import evaluate

    seqeval = evaluate.load("seqeval")
    # 获取分类
    label_list = dataset["train"].features["ner_tags"].feature.names

    def compute_metrics(p):
        predictions, labels = p
        predictions = np.argmax(
            predictions, axis=2
        )  # 维度：[batch_size, seq_len, num_labels]
        # 预测的结果
        true_predictions = [
            [label_list[p] for (p, l) in zip(prediction, label) if l != -100]
            for prediction, label in zip(predictions, labels)
        ]
        # 实际的结果
        true_labels = [
            [label_list[l] for (p, l) in zip(prediction, label) if l != -100]
            for prediction, label in zip(predictions, labels)
        ]
        # 计算指标
        results = seqeval.compute(
            predictions=true_predictions,
            references=true_labels,
        )
        return {
            "precision": results["overall_precision"],
            "recall": results["overall_recall"],
            "f1": results["overall_f1"],
            "acc": results["overall_accuracy"],
        }

    # 标签映射
    idx2tag = {idx: label_list[idx] for idx in range(len(label_list))}
    tag2idx = {label_list[idx]: idx for idx in range(len(label_list))}

    # 模型训练
    from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer

    model = AutoModelForTokenClassification.from_pretrained(
        model_name,
        cache_dir=model_path,
        num_labels=len(label_list),
        id2label=idx2tag,
        label2id=tag2idx,
        ignore_mismatched_sizes=True, # 忽略不匹配的尺寸
    )

    # 定义训练参数
    output_dir = os.path.join(CURRENT_PATH, "output")
    logging_dir = os.path.join(CURRENT_PATH, "logs")
    training_args = TrainingArguments(
        output_dir=output_dir,
        num_train_epochs=100,
        per_device_train_batch_size=16,  # batch size per device during training
        per_device_eval_batch_size=16,  # batch size for evaluation
        warmup_steps=500,
        learning_rate=2e-5,  # initial learning rate (AdamW)
        weight_decay=0.01,  # coefficient for L2 regularization
        logging_dir=logging_dir,
        logging_steps=100,
        eval_strategy="epoch",
        save_strategy="epoch",
        load_best_model_at_end=True,
    )

    # 创建训练实例对象
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=tokenized_datasets["train"],
        eval_dataset=tokenized_datasets["test"],
        compute_metrics=compute_metrics,
        tokenizer=tokenizer,
        data_collator=data_collator,
    )
    # 关闭警告信息
    warnings.filterwarnings("ignore")
    # 开始训练
    trainer.train()
```

效果：

```bash
{'loss': 0.0845, 'grad_norm': 2.1226353645324707, 'learning_rate': 1.8750000000000002e-05, 'epoch': 8.45}  
{'loss': 0.0817, 'grad_norm': 1.3901183605194092, 'learning_rate': 1.8653846153846157e-05, 'epoch': 8.92}
{'eval_loss': 0.4778915345668793, 'eval_precision': 0.24040066777963273, 'eval_recall': 0.1334569045412419, 'eval_f1': 0.17163289630512515, 'eval_acc': 0.9226297341198598, 'eval_runtime': 3.3062, 'eval_samples_per_second': 389.267, 'eval_steps_per_second': 24.499, 'epoch': 9.0}
{'loss': 0.0783, 'grad_norm': 1.2513377666473389, 'learning_rate': 1.855769230769231e-05, 'epoch': 9.39}          
{'loss': 0.0714, 'grad_norm': 1.4975522756576538, 'learning_rate': 1.8461538461538465e-05, 'epoch': 9.86}
{'eval_loss': 0.48947033286094666, 'eval_precision': 0.24579831932773108, 'eval_recall': 0.10843373493975904, 'eval_f1': 0.1504823151125402, 'eval_acc': 0.9239121142173207, 'eval_runtime': 3.3304, 'eval_samples_per_second': 386.445, 'eval_steps_per_second': 24.322, 'epoch': 10.0}
{'loss': 0.0624, 'grad_norm': 1.6514416933059692, 'learning_rate': 1.8365384615384617e-05, 'epoch': 10.33}        
{'loss': 0.0639, 'grad_norm': 1.7953861951828003, 'learning_rate': 1.826923076923077e-05, 'epoch': 10.8}
{'eval_loss': 0.5223231911659241, 'eval_precision': 0.2548262548262548, 'eval_recall': 0.12233549582947173, 'eval_f1': 0.16530995616781463, 'eval_acc': 0.922886210139352, 'eval_runtime': 3.3262, 'eval_samples_per_second': 386.924, 'eval_steps_per_second': 24.352, 'epoch': 11.0}
```



### 1.6 模型推理

```python
import os
from transformers import AutoTokenizer, AutoModelForTokenClassification
from transformers import pipeline

# 加载 tokenizer 和模型
CURRENT_PATH = os.path.dirname(os.path.abspath(__file__))
model_path = os.path.join(CURRENT_PATH, "output/checkpoint-2769")
tokenizer = AutoTokenizer.from_pretrained(model_path)
model = AutoModelForTokenClassification.from_pretrained(model_path)

ner_pipeline = pipeline(
    "ner",
    model=model,
    tokenizer=tokenizer,
    device=0,  # 使用 GPU（如果是 CPU 则设为 -1）
)

# 示例推理
text = "The Golden State Warriors are an American professional basketball team based in San Francisco.",
results = ner_pipeline(text)
print(results)
```

运行结果：

```bash
[{'entity': 'I-group', 'score': 0.9756389, 'index': 1, 'word': 'The', 'start': 0, 'end': 3}, {'entity': 'I-group', 'score': 0.9745677, 'index': 2, 'word': 'Golden', 'start': 4, 'end': 10}, {'entity': 'I-group', 'score': 0.60328686, 'index': 3, 'word': 'State', 'start': 11, 'end': 16}, {'entity': 'I-location', 'score': 0.56003207, 'index': 12, 'word': 'in', 'start': 77, 'end': 79}, {'entity': 'I-location', 'score': 0.9132929, 'index': 13, 'word': 'San', 'start': 80, 'end': 83}]
```



