# ä¸€ã€å¤§æ¨¡å‹

T5\BERT\GPT â†’ Transformerçš„å„¿å­â†’è‡ªæ³¨æ„åŠ›æœºåˆ¶+ç¥ç»ç½‘ç»œ

å¤§æ¨¡å‹ï¼Œ Large Modelï¼Œæ˜¯æŒ‡å‚æ•°è§„æ¨¡åºå¤§ã€è®­ç»ƒæ•°æ®é‡å·¨å¤§ã€å…·æœ‰å¼ºæ³›åŒ–èƒ½åŠ›çš„äººå·¥æ™ºèƒ½æ¨¡å‹ï¼Œå…¸å‹ä»£è¡¨å¦‚GPTã€BERTã€PaLMç­‰ã€‚å®ƒä»¬é€šå¸¸åŸºäºæ·±åº¦ç¥ç»ç½‘ç»œï¼Œç‰¹åˆ«æ˜¯Transformeræ¶æ„ï¼Œåœ¨è‡ªç„¶è¯­è¨€å¤„ç†ã€å›¾åƒè¯†åˆ«ã€ä»£ç ç”Ÿæˆç­‰ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ã€‚



## 1.  åŸºæœ¬æ¦‚å¿µ

<img src="media/image-20250420202934509.png" alt="image-20250420202934509" style="zoom:33%;" />

å¤§æ¨¡å‹æ˜¯æŒ‡åœ¨è¶…å¤§è§„æ¨¡æ•°æ®é›†ä¸Šè®­ç»ƒã€æ‹¥æœ‰æ•°åäº¿åˆ°åƒäº¿ä»¥ä¸Šå‚æ•°çš„äººå·¥æ™ºèƒ½æ¨¡å‹ï¼Œå…·å¤‡å¤šä»»åŠ¡ã€å¤šæ¨¡æ€èƒ½åŠ›ï¼Œå¹¶èƒ½é€šè¿‡å°‘é‡æ ·æœ¬ç”šè‡³é›¶æ ·æœ¬å®Œæˆæ–°ä»»åŠ¡ã€‚

## 2. ä¸»è¦ç‰¹å¾

å¤§æ¨¡å‹å…·å¤‡ä»¥ä¸‹ç‰¹å¾ï¼š

- å‚æ•°è§„æ¨¡å¤§
   æ•°äº¿è‡³æ•°åƒäº¿å‚æ•°ï¼Œå®¹é‡å†³å®šäº†æ¨¡å‹çš„è¡¨è¾¾ä¸æ³›åŒ–èƒ½åŠ›ã€‚
- æ•°æ®è®­ç»ƒé‡å¤§
   åˆ©ç”¨æµ·é‡æ–‡æœ¬ã€å›¾åƒã€éŸ³é¢‘ç­‰å¤šæ¨¡æ€æ•°æ®è®­ç»ƒï¼Œæå‡æ¨¡å‹çš„é€šç”¨æ€§ã€‚
- åŸºäºTransformer
   å¤šæ•°å¤§æ¨¡å‹é‡‡ç”¨Transformerä½œä¸ºåŸºç¡€ç»“æ„ï¼Œå…·å¤‡å¼ºå¤§çš„è¡¨ç¤ºå­¦ä¹ èƒ½åŠ›ã€‚
- å¼ºæ³›åŒ–ä¸è¿ç§»èƒ½åŠ›
   ä¸€æ¬¡è®­ç»ƒï¼Œå¤šåœºæ™¯å¤ç”¨ï¼ˆå¦‚ChatGPTæ—¢èƒ½å¯¹è¯ï¼Œä¹Ÿèƒ½å†™ä»£ç ã€æ”¹æ–‡æ¡ˆï¼‰ã€‚
- å…·å¤‡â€œæ¶Œç°èƒ½åŠ›â€
   æ¨¡å‹è§„æ¨¡çªç ´æŸä¸ªé˜ˆå€¼åï¼Œè¡¨ç°å‡ºè¶…è¶Šè®­ç»ƒç›®æ ‡çš„æ™ºèƒ½è¡Œä¸ºï¼ˆå¦‚é€»è¾‘æ¨ç†ã€å¤æ‚ç”Ÿæˆï¼‰ã€‚
- å¯è°ƒä¼˜èƒ½åŠ›å¼º
   æ”¯æŒå¾®è°ƒï¼ˆFine-tuningï¼‰ã€æç¤ºå­¦ä¹ ï¼ˆPrompt Learningï¼‰ã€å‚æ•°é«˜æ•ˆè°ƒä¼˜ï¼ˆLoRAã€Adapterç­‰ï¼‰ã€‚
- å¤šæ¨¡æ€å‘å±•è¶‹åŠ¿
   ä»çº¯æ–‡æœ¬æ¨¡å‹å‘å±•åˆ°å›¾æ–‡ã€è¯­éŸ³ã€è§†é¢‘ç­‰å¤šæ¨¡æ€å¤§æ¨¡å‹ï¼Œå¦‚GPT-4Vã€DeepSeekã€Grokç­‰



## 3. åº”ç”¨æ–¹å‘
1. **Agentæ™ºèƒ½ä½“ï¼ˆAIç®¡å®¶ï¼‰**
   - **æ¦‚å¿µ**ï¼šåƒä¸€ä¸ªâ€œå…¨èƒ½ç§äººåŠ©ç†â€ï¼Œèƒ½è‡ªå·±åˆ†æé—®é¢˜ã€æ‹†è§£ä»»åŠ¡ã€è°ƒç”¨å·¥å…·è§£å†³é—®é¢˜ã€‚
   - **ä¾‹å­**ï¼šä½ æƒ³å‘¨æœ«å»éœ²è¥ï¼Œå‘Šè¯‰å®ƒï¼šâ€œå¸®æˆ‘æ‰¾ä¸ªç¦»å¸‚åŒºè¿‘ã€èƒ½å¸¦å® ç‰©ã€æœ‰çƒ§çƒ¤åŒºçš„éœ²è¥åœ°ï¼Œè®¢å‘¨å…­çš„æˆ¿é—´ï¼Œå†æ¨èé™„è¿‘è¶…å¸‚ä¹°é£Ÿæã€‚â€å®ƒè‡ªåŠ¨å®Œæˆï¼šæœæ”»ç•¥ â†’ ç­›é€‰åœ°ç‚¹ â†’ æŸ¥å¤©æ°” â†’ è®¢è¥åœ° â†’ ç”Ÿæˆè´­ç‰©æ¸…å• â†’ å‘åˆ°ä½ æ‰‹æœºã€‚
   - **å¥½å¤„**ï¼šä¸ç”¨è‡ªå·±ä¸€æ­¥æ­¥æ“ä½œï¼ŒAIèƒ½â€œåŠ¨è„‘å­â€å¸®ä½ æå®šå¤æ‚ä»»åŠ¡ã€‚
   - **æµç¨‹**ï¼šç”¨æˆ·è¾“å…¥ -> æ‹†è§£ä»»åŠ¡ -> æ„å›¾è¯†åˆ« -> è°ƒç”¨å¯¹åº”çš„å‡½æ•°å¹¶æ‰§è¡Œ -> å®Œæˆæ‰§è¡Œ

2. **è¯­éŸ³èŠå¤©åŠ©æ‰‹ï¼ˆä¼šèŠå¤©çš„AIï¼‰**
   - **æ¦‚å¿µ**ï¼šåƒâ€œå‡çº§ç‰ˆSiriâ€ï¼Œèƒ½è‡ªç„¶å¯¹è¯ã€ç†è§£è¯­æ°”ï¼Œç”šè‡³æ¨¡æ‹ŸçœŸäººæƒ…æ„Ÿã€‚
   - **ä¾‹å­**ï¼šä½ å¼€è½¦æ—¶è¯´ï¼šâ€œæˆ‘å¥½å›°å•Šï¼Œæ¥ç‚¹æç¥çš„éŸ³ä¹ï¼Œå†å¯¼èˆªåˆ°æœ€è¿‘çš„å’–å•¡åº—ã€‚â€å®ƒå›ç­”ï¼šâ€œé©¬ä¸Šåˆ‡åˆ°æ‘‡æ»šæ­Œå•ï¼å‰æ–¹500ç±³æœ‰æ˜Ÿå·´å…‹ï¼Œè¦å¸®ä½ ç‚¹ä¸€æ¯å†°ç¾å¼å—ï¼Ÿâ€ï¼ˆè¿˜èƒ½å­¦ä½ å–œæ¬¢çš„è¯´è¯é£æ ¼ï¼‰
   - **å¥½å¤„**ï¼šä¸ç”¨æ‰“å­—ï¼ŒåŠ¨åŠ¨å˜´å°±èƒ½èŠå¤©ã€æŸ¥ä¿¡æ¯ã€æ§åˆ¶æ™ºèƒ½å®¶å±…ï¼Œåƒæœ‰ä¸ªâ€œéšèº«é™ªèŠâ€ã€‚
   - **æµç¨‹**ï¼šè¯­éŸ³è¾“å…¥ -> è¯­éŸ³è¯†åˆ«è½¬æ–‡å­— -> å¤§æ¨¡å‹å¯¹è¯ -> æ–‡æœ¬è½¬è¯­éŸ³ -> å®Œæˆå¯¹è¯

3. **åŒ»å­¦å®¢æœï¼ˆAIå¥åº·å°åŠ©æ‰‹ï¼‰**
   - **æ¦‚å¿µ**ï¼šåŒ»é™¢çš„â€œè™šæ‹Ÿå‰å°â€ï¼Œèƒ½è§£ç­”å¸¸è§é—®é¢˜ã€æé†’ç”¨è¯ã€åˆ†è¯Šå»ºè®®ã€‚
   - **ä¾‹å­**ï¼šä½ åŠå¤œèƒƒç–¼ï¼Œæ‰“å¼€åŒ»é™¢APPé—®ï¼šâ€œåƒäº†ç«é”…åèƒƒç—›ï¼Œè¯¥æŒ‚å“ªä¸ªç§‘ï¼Ÿç°åœ¨èƒ½åƒä»€ä¹ˆè¯ç¼“è§£ï¼Ÿâ€AIå›ç­”ï¼šâ€œå»ºè®®æŒ‚æ¶ˆåŒ–å†…ç§‘ï¼Œæš‚æ—¶å¯æœç”¨XXè¯ï¼ˆéå¤„æ–¹ï¼‰ã€‚è‹¥å‘•ååŠ é‡ï¼Œè¯·ç«‹å³æ€¥è¯Šã€‚â€å¹¶æ¨é€é™„è¿‘24å°æ—¶è¯åº—ã€‚
   - **å¥½å¤„**ï¼š24å°æ—¶åœ¨çº¿ï¼Œå¿«é€Ÿè§£ç­”å°æ¯›ç—…ï¼Œé¿å…æ’é˜Ÿé—®åŒ»ç”Ÿï¼Œéšç§é—®é¢˜ä¹Ÿèƒ½åŒ¿åå’¨è¯¢ã€‚
   - **æµç¨‹**ï¼šé—®é¢˜è¾“å…¥ -> æ£€ç´¢çŸ¥è¯†åº“ -> é—®é¢˜æ‹¼æ¥ -> å¤§æ¨¡å‹å¯¹è¯ -> ç»™äºˆå›å¤

## 4. å¼€å‘æµç¨‹

<img src="media/image-20250420203018771.png" alt="image-20250420203018771" style="zoom: 67%;" />

å¤§æ¨¡å‹å¼€å‘æ˜¯ä¸€ä¸ªç³»ç»Ÿå·¥ç¨‹ï¼Œæ¶‰åŠæ•°æ®ã€æ¨¡å‹ã€ç®—åŠ›ã€è®­ç»ƒã€éƒ¨ç½²ã€å®‰å…¨ä¸è¿­ä»£ç­‰å¤šä¸ªç¯èŠ‚ã€‚

- **ä»»åŠ¡å®šä¹‰ä¸éœ€æ±‚åˆ†æ**
  - æ˜ç¡®åº”ç”¨åœºæ™¯ï¼ˆå¦‚å¯¹è¯ã€å†™ä½œã€æ¨èã€å›¾åƒè¯†åˆ«ç­‰ï¼‰
  - é€‰æ‹©æ¨¡å‹ç±»å‹ï¼ˆNLPã€CVã€å¤šæ¨¡æ€ç­‰ï¼‰
- **æ•°æ®å‡†å¤‡ä¸é¢„å¤„ç†**
  - æ”¶é›†é«˜è´¨é‡ã€å¤§è§„æ¨¡æ•°æ®ï¼ˆæ–‡æœ¬ã€å›¾åƒã€éŸ³é¢‘ç­‰ï¼‰
  - å»å™ªæ¸…æ´—ã€æ ‡æ³¨ã€æ ¼å¼è½¬æ¢ã€å»é‡ä¸åˆ†è¯ç­‰
- **æ¨¡å‹è®¾è®¡ä¸é€‰æ‹©**
  - é€‰æ‹©åˆé€‚çš„æ¨¡å‹æ¶æ„ï¼ˆå¦‚GPTã€BERTã€ViTã€T5ç­‰ï¼‰
  - è®¾å®šå±‚æ•°ã€å®½åº¦ã€æ³¨æ„åŠ›æœºåˆ¶ç­‰ç»“æ„å‚æ•°
- **è®­ç»ƒç­–ç•¥ä¸èµ„æºé…ç½®**
  - åˆ†å¸ƒå¼è®­ç»ƒ/æ··åˆç²¾åº¦è®­ç»ƒ
  - ä½¿ç”¨å¤§è§„æ¨¡ç®—åŠ›èµ„æºï¼ˆGPU/TPUé›†ç¾¤ï¼‰
  - è®¾ç½®ä¼˜åŒ–å™¨ï¼ˆAdamWï¼‰ã€å­¦ä¹ ç‡è°ƒåº¦ç­‰å‚æ•°
- **è¯„ä¼°ä¸è°ƒä¼˜**
  - è¯„ä¼°æŒ‡æ ‡ï¼šPPLã€å‡†ç¡®ç‡ã€BLEUã€ROUGEã€F1ç­‰
  - å¾®è°ƒ/æŒ‡ä»¤è°ƒä¼˜ï¼ˆInstruction tuningï¼‰/RLHFç­‰æ–¹æ³•æå‡æ•ˆæœ
- **æ¨ç†éƒ¨ç½²ä¸å‹ç¼©ä¼˜åŒ–**
  - éƒ¨ç½²åˆ°æœåŠ¡å™¨æˆ–è¾¹ç¼˜ç«¯ï¼ˆäº‘éƒ¨ç½²ã€APIæœåŠ¡ï¼‰
  - æ¨¡å‹é‡åŒ–ã€è£å‰ªã€è’¸é¦ã€MoEç­‰æ‰‹æ®µæå‡æ¨ç†æ•ˆç‡
- **å®‰å…¨æœºåˆ¶ä¸åˆè§„æ£€æµ‹**
  - é˜²æ­¢ç”Ÿæˆæœ‰å®³/æ•æ„Ÿå†…å®¹
  - å¯¹è¾“å‡ºè¿›è¡Œå†…å®¹å®¡æŸ¥ã€å¯¹æŠ—æ ·æœ¬é˜²å¾¡ã€æ¨¡å‹æ°´å°ç­‰
- **æŒç»­è¿­ä»£ä¸ç”Ÿæ€æ„å»º**
  - åŸºäºç”¨æˆ·åé¦ˆæŒç»­ä¼˜åŒ–
  - æ„å»ºæ’ä»¶ç³»ç»Ÿã€å¼€å‘è€…å¹³å°ç­‰ç”Ÿæ€ä½“ç³»

## 5. å…³é”®è¦ç‚¹

- åœ¨å¤§æ¨¡å‹åº”ç”¨å¼€å‘æ—¶ï¼Œä¸ä¼šä»å¤´å¼€å§‹æ„å»ºä¸€ä¸ªæ–°çš„æ¨¡å‹ï¼ŒåŸºäºå·²æœ‰åŸºåº§æ¨¡å‹è¿›è¡ŒäºŒæ¬¡å¼€å‘æ˜¯è¡Œä¸šä¸»æµå®è·µã€‚

- é€‰ç”¨å·²æœ‰çš„åŸºåº§æ¨¡å‹ï¼Œå¹¶é‡‡ç”¨ç›¸åº”çš„æŠ€æœ¯æ‰‹æ®µä¼˜åŒ–å¤§æ¨¡å‹ï¼Œå¦‚ï¼šå¾®è°ƒï¼ŒRAGï¼Œå¹¶è¡Œæ¨ç†ç­‰ã€‚
- é€‰ç”¨æµè¡Œä¸”æˆç†Ÿçš„æ¡†æ¶ï¼Œé€šè¿‡å‚æ•°è°ƒæ•´å’ŒåŠŸèƒ½é›†æˆå®ç°ä¸šåŠ¡éœ€æ±‚ï¼Œé¿å…é‡å¤é€ è½®å­ã€‚

## 6. é¡¹ç›®ä»‹ç»
- ç®€å†

  <img src="media/image-20250420204539854.png" alt="image-20250420204539854" style="zoom: 50%;" />

- é¡¹ç›®æ¡†æ¶

  <img src="media/image-20250420203934045.png" alt="image-20250420203934045" style="zoom: 80%;" />

- ä¸‰ä¸ªæ¨¡å—
   - å¤§è¯­è¨€æ¨¡å‹(LLM)--å®ç°å¤§è¯­è¨€æ¨¡å‹çš„å¯¹è¯åŠŸèƒ½ï¼šå¤§æ¨¡å‹çš„æœ¬åœ°éƒ¨ç½²ï¼Œå¾®è°ƒè®­ç»ƒï¼Œæµå¼å¯¹è¯ï¼Œå¤šè½®å¯¹è¯ã€‚
   - RAGæŠ€æœ¯--å¢å¼ºå¤§è¯­è¨€æ¨¡å‹çš„å¯¹è¯èƒ½åŠ›ï¼šæ–‡æœ¬åˆ†å‰²ï¼Œæ–‡æœ¬åµŒå…¥ï¼ŒçŸ¥è¯†æ£€ç´¢
   - çŸ¥è¯†å›¾è°±--å®ç°æ•°æ®çš„é«˜æ•ˆå­˜å‚¨ï¼šneo4jæ•°æ®åº“ï¼Œä¸‰å…ƒç»„æŠ½å–ï¼Œå®ä½“å¯¹é½

- é¡¹ç›®å±•ç¤º

  | <img src="media/image-20250420204155809.png" alt="image-20250420204155809" style="zoom:67%;" /> |
  | ------------------------------------------------------------ |
  | <img src="media/image-20250420204238357.png" alt="image-20250420204238357" style="zoom:58%;" /> |

  

# äºŒã€LLM

å¼€å§‹å®é™…æ¡ˆä¾‹çš„å±•ç¤º......

## 1. APIè°ƒç”¨

å¤§æ¨¡å‹é€šè¿‡APIè°ƒç”¨æ˜¯ç›®å‰æœ€å¸¸è§ã€æœ€ä¾¿æ·çš„ä½¿ç”¨æ–¹å¼ï¼Œç”¨æˆ·æ— éœ€è®­ç»ƒæ¨¡å‹ï¼Œåªéœ€è°ƒç”¨æ¥å£å³å¯äº«å—å¼ºå¤§çš„ AI èƒ½åŠ›ï¼Œæ¯”å¦‚æ–‡æœ¬ç”Ÿæˆã€ç¿»è¯‘ã€å›¾åƒè¯†åˆ«ã€ä»£ç è¡¥å…¨ç­‰ã€‚

### 1.1 åŸºæœ¬æµç¨‹

- **è·å– API æƒé™**
  - æ³¨å†Œå¹³å°è´¦å·ï¼ˆå¦‚ OpenAIã€DeepSeekã€é˜¿é‡Œé€šä¹‰ã€è®¯é£æ˜Ÿç«ç­‰ï¼‰
  - è·å– `API Key` æˆ– `Access Token`
- **å‡†å¤‡è¯·æ±‚å‚æ•°**
  - é€‰æ‹©æ¨¡å‹
  - è®¾ç½®è¯·æ±‚ä½“
- **å‘èµ· API è¯·æ±‚**
  - ä½¿ç”¨ç¼–ç¨‹è¯­è¨€ï¼ˆå¦‚ Pythonã€JavaScriptï¼‰é€šè¿‡ HTTP åè®®è°ƒç”¨æ¥å£
- **è§£æå“åº”ç»“æœ**
  - è·å–æ¨¡å‹è¿”å›å†…å®¹ï¼ˆå¦‚æ–‡æœ¬ã€å›¾ç‰‡é“¾æ¥ã€ç»“æ„åŒ–æ•°æ®ç­‰ï¼‰
  - å¯ä¸å‰ç«¯ã€åº”ç”¨ç³»ç»Ÿé›†æˆä½¿ç”¨

### 1.2 åŸºæœ¬ç‰¹å¾

<img src="media/image-20250420205345161.png" alt="image-20250420205345161" style="zoom: 67%;" />

å¤§æ¨¡å‹APIè°ƒç”¨å°†å¤æ‚çš„æ¨¡å‹èƒ½åŠ›ç®€åŒ–ä¸ºæ ‡å‡†åŒ–æœåŠ¡ï¼Œæ ¸å¿ƒä»·å€¼åœ¨äºï¼š

- é™ä½ä½¿ç”¨é—¨æ§›ï¼šæ— éœ€æœ¬åœ°éƒ¨ç½²åƒäº¿å‚æ•°æ¨¡å‹ï¼ŒèŠ‚çœç¡¬ä»¶ä¸è¿ç»´æˆæœ¬ã€‚

- çµæ´»é€‚é…åœºæ™¯ï¼šé€šè¿‡å‚æ•°è°ƒèŠ‚å’Œä¸Šä¸‹æ–‡å­¦ä¹ å¿«é€Ÿæ»¡è¶³ä¸šåŠ¡éœ€æ±‚ã€‚

- è§„æ¨¡åŒ–æ”¯æŒï¼šä¾æ‰˜äº‘è®¡ç®—å®ç°é«˜å¯ç”¨ã€ä½å»¶è¿Ÿçš„ä¼ä¸šçº§æœåŠ¡ã€‚



### 1.3 DeepSeekå®æ“

DeepSeekä½œä¸ºå›½å†…ä¼˜ç§€çš„LLMå¹³å°ï¼Œæ˜¯ä¸€ä¸ªä¸é”™çš„é€‰æ‹©~

#### 1.3.1 å‡†å¤‡å·¥ä½œ

- è®¿é—®deepseekå®˜ç½‘ï¼Œå¹¶æ³¨å†Œè´¦å·ï¼š[DeepSeekå®˜ç½‘](https://www.deepseek.com/)

  <img src="media/image-20250420205637779.png" alt="image-20250420205637779" style="zoom:67%;" />

  

- æ³¨å†Œè´¦å·å¹¶ä¸”å……å€¼

  <img src="media/image-20250420205700220.png" alt="image-20250420205700220" style="zoom:67%;" />

- åˆ›å»ºAPI-key

  ==ä»…åœ¨åˆ›å»ºæ—¶å¯è§å¯å¤åˆ¶==

  <img src="media/image-20250420210334465.png" alt="image-20250420210334465" style="zoom:42%;" />

  ```bash
  sk-f9deff6faca64899a3faaaf1f4c53d1d
  ```

  

- æŸ¥çœ‹ä½¿ç”¨æ‰‹å†Œ

  <img src="media/image-20250420210615821.png" alt="image-20250420210615821" style="zoom:60%;" />

  

#### 1.3.2 éæµå¼è¾“å‡º

 ç­‰æ¨¡å‹ç”Ÿæˆå®Œæ•´ç»“æœåä¸€æ¬¡æ€§è¿”å›ï¼Œé€‚åˆçŸ­æ–‡æœ¬ã€ç»“æ„åŒ–å†…å®¹æå–ç­‰ä»»åŠ¡ã€‚

- ç‰¹ç‚¹

  - ä¼˜ç‚¹ï¼š

    - ä½¿ç”¨ç®€å•ï¼Œ**ä¸€æ¬¡æ€§æ‹¿åˆ°å®Œæ•´ç»“æœ**

    - é€‚åˆåˆ†æå¤„ç†ã€æ‘˜è¦æŠ½å–ã€çŸ­æ–‡æœ¬é—®ç­”ç­‰

  - ç¼ºç‚¹ï¼š

    - å“åº”æ—¶é—´é•¿ï¼Œç‰¹åˆ«æ˜¯æ–‡æœ¬å¾ˆé•¿æ—¶

    - ä½“éªŒè¾ƒå·®ï¼Œç”¨æˆ·éœ€è¦ç­‰å¾…å…¨éƒ¨ç”Ÿæˆå®Œæ‰èƒ½çœ‹åˆ°å†…å®¹

- ç¯å¢ƒå®‰è£…
   ```bash
   pip install openai
   ```
- éæµå¼è¾“å‡º
   ```python
   # Please install OpenAI SDK first: `pip3 install openai`
   from openai import OpenAI
   
   client = OpenAI(
       api_key="sk-f9deff6faca64899a3faaaf1f4c53d1d", base_url="https://api.deepseek.com"
   )
   
   response = client.chat.completions.create(
       model="deepseek-chat",
       messages=[
           {"role": "system", "content": "You are a helpful assistant"},
           {"role": "user", "content": "æ˜æœˆå‡ æ—¶æœ‰"},
       ],
       stream=False,
   )
   
   print(response.choices[0].message.content)
   
   ```

- è¾“å‡ºç»“æœ

  ```bash
  â€œæ˜æœˆå‡ æ—¶æœ‰â€æ˜¯è‹è½¼ã€Šæ°´è°ƒæ­Œå¤´Â·æ˜æœˆå‡ æ—¶æœ‰ã€‹ä¸­çš„åå¥ï¼Œå…¨æ–‡å¦‚ä¸‹ï¼š
  
  **ã€Šæ°´è°ƒæ­Œå¤´Â·æ˜æœˆå‡ æ—¶æœ‰ã€‹**  
  æ˜æœˆå‡ æ—¶æœ‰ï¼ŸæŠŠé…’é—®é’å¤©ã€‚  
  ä¸çŸ¥å¤©ä¸Šå®«é˜™ï¼Œä»Šå¤•æ˜¯ä½•å¹´ã€‚  
  æˆ‘æ¬²ä¹˜é£å½’å»ï¼Œåˆæç¼æ¥¼ç‰å®‡ï¼Œé«˜å¤„ä¸èƒœå¯’ã€‚  
  èµ·èˆå¼„æ¸…å½±ï¼Œä½•ä¼¼åœ¨äººé—´ã€‚  
  
  è½¬æœ±é˜ï¼Œä½ç»®æˆ·ï¼Œç…§æ— çœ ã€‚
  ä¸åº”æœ‰æ¨ï¼Œä½•äº‹é•¿å‘åˆ«æ—¶åœ†ï¼Ÿ
  äººæœ‰æ‚²æ¬¢ç¦»åˆï¼Œæœˆæœ‰é˜´æ™´åœ†ç¼ºï¼Œæ­¤äº‹å¤éš¾å…¨ã€‚
  ä½†æ„¿äººé•¿ä¹…ï¼Œåƒé‡Œå…±å©µå¨Ÿã€‚
  
  ### èµæï¼š
  1. **èƒŒæ™¯**ï¼šæ­¤è¯ä½œäºå®‹ç¥å®—ç†™å®ä¹å¹´ï¼ˆ1076å¹´ï¼‰ä¸­ç§‹ï¼Œè‹è½¼åœ¨å¯†å·ï¼ˆä»Šå±±ä¸œè¯¸åŸï¼‰ä»»èŒæ—¶ï¼Œæ€€å¿µå¼Ÿå¼Ÿè‹ è¾™è€Œå†™ã€‚
  2. **æƒ…æ„Ÿ**ï¼šä»¥æœˆèµ·å…´ï¼Œå›´ç»•ä¸­ç§‹æ˜æœˆå±•å¼€æƒ³è±¡ï¼Œäº¤ç»‡äººé—´æƒ…æ€€ä¸å®‡å®™å“²æ€ï¼Œæ—¢æœ‰å¯¹äº²äººçš„æ€å¿µï¼Œåˆæœ‰å¯¹ äººç”Ÿæ— å¸¸çš„è±è¾¾ã€‚
  3. **åå¥**ï¼š
     - â€œäººæœ‰æ‚²æ¬¢ç¦»åˆï¼Œæœˆæœ‰é˜´æ™´åœ†ç¼ºâ€é“å‡ºä¸–äº‹æ— å¸¸çš„å¸¸æ€ã€‚
     - â€œä½†æ„¿äººé•¿ä¹…ï¼Œåƒé‡Œå…±å©µå¨Ÿâ€æˆä¸ºè¡¨è¾¾è¿œæ–¹äº²å‹å¹³å®‰å…±å‹‰çš„åƒå¤ç»å”±ã€‚
  
  
  ### å°çŸ¥è¯†ï¼š
  è¿™é¦–è¯è¢«èª‰ä¸ºâ€œä¸­ç§‹è¯ä¹‹å† â€ï¼Œåæ›¾è¢«æ”¹ç¼–ä¸ºç»å…¸æ­Œæ›²ï¼ˆå¦‚ç‹è²æ¼”å”±çš„ã€Šä½†æ„¿äººé•¿ä¹…ã€‹ï¼‰ã€‚è‹¥æ‚¨æƒ³è¿›ä¸€æ­¥æ¢è®¨å…¶æ–‡å­¦æ‰‹æ³•æˆ–åˆ›ä½œèƒŒæ™¯ï¼Œå¯ä»¥éšæ—¶å‘Šè¯‰æˆ‘ï¼
  ```
  
  

#### 1.3.3 æµå¼è¾“å‡º

 æœåŠ¡å™¨å°†å“åº”å†…å®¹ä¸€æ®µä¸€æ®µåœ°å®æ—¶è¿”å›ï¼Œé€‚åˆé•¿æ–‡æœ¬ã€å¯¹è¯ã€å†™ä½œç­‰éœ€è¦å³æ—¶åé¦ˆçš„åœºæ™¯ã€‚

- ç‰¹ç‚¹

  - ä¼˜ç‚¹

    - **å“åº”å¿«**ï¼šæ— éœ€ç­‰å…¨éƒ¨ç”Ÿæˆå®Œæ¯•ï¼Œå…ˆç”Ÿæˆå…ˆè¿”å›

    - **ä½“éªŒä½³**ï¼šåƒäººæ‰“å­—ä¸€æ ·æµç•…ï¼Œå¸¸ç”¨äºå¯¹è¯æœºå™¨äºº

    - **å¯ä¸­æ–­**ï¼šç”¨æˆ·å¯éšæ—¶æ‰“æ–­æµå¼å“åº”è¿‡ç¨‹

  - ç¼ºç‚¹

    - ç¼–ç¨‹ç¨å¤æ‚ï¼Œéœ€è¦å¤„ç†æ•°æ®æµæ‹¼æ¥

    - ä¸æ˜“ç›´æ¥ä½¿ç”¨æ™®é€š HTTP è¯·æ±‚å·¥å…·ï¼ˆå¦‚ Postmanï¼‰

- åŸç†

  <img src="media/image-20250420211402987.png" alt="image-20250420211402987" style="zoom:50%;" />

- æµå¼æ¨ç†çš„å®ç°â€”ç”Ÿæˆå™¨
   ```python
   import time
   
   def test():  # ç”Ÿæˆå™¨å‡½æ•°
       for i in range(10):
           time.sleep(1)
           yield i  # ç”Ÿæˆå™¨å‡½æ•°ï¼Œä½¿ç”¨yieldå…³é”®å­—è¿”å›å€¼
   
   if __name__ == "__main__":
       aaa = test()
       print(aaa)  # aaaæ˜¯ä¸€ä¸ªç”Ÿæˆå™¨ï¼Œå¯ä»¥æƒ³è±¡æˆä¸€ä¸ªé˜Ÿåˆ—ï¼Œæ¯è¯»å–ä¸€æ¬¡ï¼Œå°±ä¼šæ‰§è¡Œä¸€æ¬¡å‡½æ•°ä½“
       for a in aaa:
           print(a)  # è¯»å–ç”Ÿæˆå™¨ä¸­çš„å€¼
   ```

- æµå¼æ¨ç†ä»£ç ç¼–å†™ï¼š
   ```python
   from openai import OpenAI
   
   client = OpenAI(
       api_key="sk-f9deff6faca64899a3faaaf1f4c53d1d", base_url="https://api.deepseek.com"
   )
   
   response = client.chat.completions.create(
       model="deepseek-chat",
       messages=[
           {"role": "system", "content": "You are a helpful assistant"},
           {"role": "user", "content": "æ˜æœˆå‡ æ—¶æœ‰"},
       ],
       stream=True,
   )
   
   # æµå¼è¾“å‡º
   out = []
   for chunk in response:
       print(chunk.choices[0].delta.content)
       out.append(chunk.choices[0].delta.content)
       print('-' * 10)
       print(''.join(out))
   
   ```



#### 1.3.4 æ€»ç»“å¯¹æ¯”

| é¡¹ç›®       | æµå¼è¾“å‡º           | éæµå¼è¾“å‡º           |
| ---------- | ------------------ | -------------------- |
| è¿”å›æ–¹å¼   | è¾¹ç”Ÿæˆè¾¹è¿”å›       | å…¨éƒ¨ç”Ÿæˆåä¸€æ¬¡è¿”å›   |
| å“åº”é€Ÿåº¦   | å¿«                 | æ…¢ï¼ˆå°¤å…¶æ˜¯é•¿æ–‡æœ¬ï¼‰   |
| ä½¿ç”¨ä½“éªŒ   | æ›´è‡ªç„¶ï¼ˆæ‰“å­—å¼ï¼‰   | ç­‰å¾…è¿‡ç¨‹è¾ƒé•¿         |
| ç¼–ç¨‹å¤æ‚åº¦ | ç¨å¤æ‚ï¼ˆéœ€æ‹¼æ¥ï¼‰   | ç®€å•                 |
| é€‚ç”¨åœºæ™¯   | å¯¹è¯ç”Ÿæˆã€ç›´æ’­é—®ç­” | ç®€çŸ­å›å¤ã€ç»“æ„åŒ–å¤„ç† |



#### 1.3.5 ä»£ç å°è£…

å°†ä»£ç å°è£…ä¸ºç±»ï¼Œæ–¹ä¾¿å…¶ä»–æ–‡ä»¶è°ƒç”¨æ­¤åŠŸèƒ½ã€‚

**éæµå¼è¾“å‡ºï¼š**

```python
from openai import OpenAI

class DeepseekAPI:
    def __init__(self, api_key):  # åˆå§‹åŒ–æ–¹æ³•
        self.api_key = api_key  # APIå¯†é’¥
        self.client = OpenAI(
            api_key=api_key, base_url="https://api.deepseek.com"
        )  # å®ä¾‹åŒ–OpenAIå®¢æˆ·ç«¯

    def inference(self, messages):
        response = self.client.chat.completions.create(
            model="deepseek-chat",
            messages=messages,  # æ¶ˆæ¯å†…å®¹
            stream=False,  # è®¾ç½®ä¸ºFalseä»¥è·å–å®Œæ•´å“åº”
        )
        return response.choices[0].message.content  # è¿”å›å®Œæ•´å“åº”


# æµ‹è¯•ä»£ç 
if __name__ == "__main__":
    api_key = "sk-f9deff6faca64899a3faaaf1f4c53d1d"  # APIå¯†é’¥
    messages = [
        {"role": "system", "content": "ä½ æ˜¯ä¸€åAIåŠ©æ‰‹"},
        {"role": "user", "content": "è¯·ç®€è¦ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±"},
    ]  # å®šä¹‰æ¶ˆæ¯å†…å®¹
    stream = False  # è®¾ç½®ä¸ºTrueä»¥è·å–æµå¼è¾“å‡ºï¼ŒFalseä»¥è·å–å®Œæ•´å“åº”
    deepseek_api = DeepseekAPI(api_key)  # å®ä¾‹åŒ–DeepseekAPIç±»
    result = deepseek_api.inference(messages)  # è°ƒç”¨æ¨ç†æ–¹æ³•
    print(result)  # æ‰“å°å“åº”å†…å®¹

```

**æµå¼è¾“å‡ºï¼š**

```python
# æµå¼è¾“å‡º:
from openai import OpenAI

class DeepseekAPI:
    def __init__(self, api_key):  # åˆå§‹åŒ–æ–¹æ³•
        self.api_key = api_key  # APIå¯†é’¥
        self.client = OpenAI(
            api_key=api_key, base_url="https://api.deepseek.com"
        )  # å®ä¾‹åŒ–OpenAIå®¢æˆ·ç«¯

    def inference(self, messages):
        response = self.client.chat.completions.create(
            model="deepseek-chat",
            messages=messages,  # æ¶ˆæ¯å†…å®¹
            stream=True,  # è®¾ç½®ä¸ºFalseä»¥è·å–å®Œæ•´å“åº”
        )
        for chunk in response:  # éå†å“åº”çš„æ¯ä¸ªå—
            if chunk.choices:  # å¦‚æœå—ä¸­æœ‰è¿”å›å†…å®¹
                content = chunk.choices[0].delta.content  # è·å–å†…å®¹
                yield content  # é€å—è¿”å›å†…å®¹

# æµ‹è¯•ä»£ç 
if __name__ == "__main__":
    api_key = "sk-f9deff6faca64899a3faaaf1f4c53d1d"  # APIå¯†é’¥
    messages = [
        {"role": "system", "content": "ä½ æ˜¯ä¸€åä¹äºåŠ©äººçš„äººå·¥æ™ºèƒ½åŠ©æ‰‹"},
        {"role": "user", "content": "è¯·ç®€è¦ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±"},
    ]  # å®šä¹‰æ¶ˆæ¯å†…å®¹
    stream = False  # # è®¾ç½®ä¸ºTrueä»¥è·å–æµå¼è¾“å‡ºï¼ŒFalseä»¥è·å–å®Œæ•´å“åº”
    deepseek_api = DeepseekAPI(api_key)  # å®ä¾‹åŒ–DeepseekAPIç±»
    result = deepseek_api.inference(messages)  # è°ƒç”¨æ¨ç†æ–¹æ³•
    for chunk in result:  # éå†å“åº”çš„æ¯ä¸ªå—
        print(chunk, end="")  # æ‰“å°æ¯ä¸ªå—çš„å†…å®¹

```



## 2.  DeepSeek-1.5Bæœ¬åœ°éƒ¨ç½²

å¤§æ¨¡å‹æœ¬åœ°éƒ¨ç½²æ˜¯æŒ‡å°†å¤§å‹é¢„è®­ç»ƒæ¨¡å‹ï¼ˆå¦‚GPTã€Llamaã€BERTç­‰ï¼‰å®Œå…¨éƒ¨ç½²åœ¨ç”¨æˆ·è‡ªæœ‰çš„ç¡¬ä»¶è®¾å¤‡ï¼ˆå¦‚æœåŠ¡å™¨ã€æœ¬åœ°è®¡ç®—æœºï¼‰ä¸Šï¼Œè€Œéä¾èµ–äº‘ç«¯APIæœåŠ¡ã€‚

### 2.1 ç‰¹ç‚¹
- ç§æœ‰åŒ–ï¼šæ¨¡å‹å’Œæ•°æ®å®Œå…¨å­˜å‚¨åœ¨æœ¬åœ°ï¼Œæ— éœ€é€šè¿‡äº’è”ç½‘ä¼ è¾“ã€‚
- è‡ªä¸»æ§åˆ¶ï¼šç”¨æˆ·æ‹¥æœ‰æ¨¡å‹çš„å®Œæ•´æƒé™ï¼Œå¯è‡ªç”±ä¿®æ”¹ã€è®­ç»ƒæˆ–è°ƒæ•´æ¨ç†é€»è¾‘ã€‚
- ç¦»çº¿è¿è¡Œï¼šæ— éœ€ç½‘ç»œè¿æ¥å³å¯ä½¿ç”¨æ¨¡å‹èƒ½åŠ›ï¼ˆå¦‚ç”Ÿæˆæ–‡æœ¬ã€åˆ†ææ•°æ®ï¼‰ã€‚



### 2.2 åŠŸèƒ½
- æ•°æ®å®‰å…¨ï¼šæ•æ„Ÿæ•°æ®ï¼ˆå¦‚åŒ»ç–—è®°å½•ã€ä¼ä¸šæœºå¯†ï¼‰æ— éœ€ä¸Šä¼ ç¬¬ä¸‰æ–¹æœåŠ¡å™¨ï¼Œé¿å…æ³„éœ²é£é™©ã€‚
- æ¨¡å‹å¾®è°ƒï¼šåŸºäºæœ¬åœ°æ•°æ®è°ƒæ•´æ¨¡å‹å‚æ•°ï¼Œé€‚é…å‚ç›´é¢†åŸŸä»»åŠ¡ï¼ˆå¦‚æ³•å¾‹åˆåŒåˆ†æï¼‰ã€‚
- ç¡¬ä»¶é€‚é…ï¼šé’ˆå¯¹æœ¬åœ°GPU/CPUèµ„æºä¼˜åŒ–æ¨¡å‹æ¨ç†é€Ÿåº¦ï¼ˆå¦‚é‡åŒ–ã€å‰ªæï¼‰ã€‚
- æ— éœ€è”ç½‘ï¼šåœ¨æ–­ç½‘ç¯å¢ƒï¼ˆå¦‚å®éªŒå®¤ã€ä¿å¯†æœºæ„ï¼‰ä¸­ä»å¯ä½¿ç”¨æ¨¡å‹èƒ½åŠ›ã€‚

### 2.3 æ¨¡å‹ä¸‹è½½
- ä»huggingfaceæ‰¾åˆ°ä½ è¦ä¸‹è½½çš„æ¨¡å‹

  ![image-20250618201852196](llm/image-20250618201852196.png)

  

- å®‰è£…huggingfaceçš„ä¸‹è½½å·¥å…·(pythonåº“)ï¼š

   ```bash
   pip install huggingface_hub
   ```

- ä¸‹è½½æ¨¡å‹æ–‡ä»¶

   ```bash
   set HF_ENDPOINT=https://hf-mirror.com  # åŠ é€Ÿä¸‹è½½è®¾ç½®
   huggingface-cli download deepseek-ai/DeepSeek-R1-0528 --local-dir F:\models  # ä¸‹è½½æ¨¡å‹æ–‡ä»¶
   ```



### 2.4 éæµå¼æ¨ç†

åˆ©ç”¨transformeræ¡†æ¶è¿›è¡Œéƒ¨ç½²æ¨ç†ï¼š
Transformeråº“çš„ä½¿ç”¨æ‰‹å†Œï¼š[Transformersæ–‡æ¡£](https://huggingface.co/docs/transformers/v4.50.0/zh/quicktour)

#### 2.4.1 å‚è€ƒä»£ç 

```python
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

# ä¸€ï¼šåŠ è½½æ¨¡å‹
model_path = r"./modeldir"  # æ¨¡å‹è·¯å¾„
device = "cuda:0" if torch.cuda.is_available() else "cpu"
torch_dtype = torch.float16  # æŒ‡å®šæ¨¡å‹å‚æ•°ç±»å‹ä¸ºfloat16
model = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=torch_dtype).to(
    device
)  # åŠ è½½æ¨¡å‹å¹¶ç§»åŠ¨åˆ°GPU
tokenizer = AutoTokenizer.from_pretrained(model_path)  # åŠ è½½åˆ†è¯å™¨

# äºŒï¼šè®¾ç½®ç”Ÿæˆå‚æ•°å’Œè¾“å…¥æ¶ˆæ¯
gen_kwargs = {
    "max_length": 1024,  # ç”Ÿæˆçš„æœ€å¤§é•¿åº¦
    "do_sample": True,  # æ˜¯å¦ä½¿ç”¨æ¦‚ç‡é‡‡æ ·
    "top_k": 10,  # é‡‡æ ·æ—¶çš„å‰Kä¸ªå€™é€‰è¯ï¼Œè¶Šå¤§è¶Šéšæœº
    "temperature": 0.7,  # ç”Ÿæˆä¸°å¯Œæ€§ï¼Œè¶Šå¤§è¶Šæœ‰åˆ›é€ åŠ›
    "top_p": 0.8,  # é‡‡æ ·æ—¶çš„å‰Pä¸ªå€™é€‰è¯ï¼Œè¶Šå¤§è¶Šéšæœº
    "repetition_penalty": 1.2,  # é‡å¤æƒ©ç½šç³»æ•°ï¼Œè¶Šå¤§è¶Šä¸å®¹æ˜“é‡å¤
}
# å®šä¹‰æ¶ˆæ¯å†…å®¹
messages = [
    {"role": "system", "content": "ä½ æ˜¯AIåŠ©æ‰‹"},
    {"role": "user", "content": "æ˜æœˆå‡ æ—¶æœ‰"},
]

# ä¸‰ï¼šå°†è¾“å…¥æ•°æ®è½¬æ¢ä¸ºæ¨¡å‹å¯æ¥å—çš„æ ¼å¼
inputs = tokenizer.apply_chat_template(
    messages,
    add_generation_prompt=True,
    tokenize=True,
    return_tensors="pt",
    return_dict=True,
).to(
    device
)  # å°†è¾“å…¥æ•°æ®ç§»åŠ¨åˆ°GPU

# å››ï¼šç”Ÿæˆè¾“å‡º
outputs = model.generate(**inputs, **gen_kwargs)  # ç”Ÿæˆè¾“å‡º
outputs = outputs[:, inputs["input_ids"].shape[1] :]  # æˆªå–ç”Ÿæˆçš„è¾“å‡º
result = tokenizer.decode(outputs[0], skip_special_tokens=True)  # è§£ç è¾“å‡º

# äº”ï¼šæ‰“å°ç»“æœ
print(result)  # æ‰“å°ç»“æœ
```

#### 2..4.2 å‚æ•°è¯¦è§£

1. **device**
   - æ¦‚å¿µï¼šæŒ‡å®šæ¨¡å‹è¿è¡Œçš„è®¡ç®—è®¾å¤‡ï¼ˆCPU æˆ– GPUï¼‰ã€‚åœ¨ PyTorch ä¸­é€šå¸¸ä¸º "cpu" æˆ– "cuda:0"ã€‚
   - è®¾ç½®å»ºè®®ï¼šä¼˜å…ˆä½¿ç”¨ GPUï¼ˆå¦‚ device="cuda:0"ï¼‰ï¼Œæ˜¾å­˜ä¸è¶³æ—¶ç”¨ CPUã€‚
2. **torch_dtype**
   - æ¦‚å¿µï¼šæ¨¡å‹å¼ é‡çš„æ•°æ®ç±»å‹ï¼Œå¦‚ float32ï¼ˆé«˜ç²¾åº¦ï¼‰ã€float16 æˆ– bfloat16ï¼ˆä½ç²¾åº¦ï¼ŒèŠ‚çœæ˜¾å­˜ï¼‰ã€‚
   - å½±å“ï¼šç²¾åº¦è¶Šé«˜ï¼ˆå¦‚ float32ï¼‰ï¼Œç»“æœè¶Šç²¾ç¡®ï¼Œä½†æ˜¾å­˜å ç”¨æ›´å¤§ã€‚ç²¾åº¦è¶Šä½ï¼ˆå¦‚ float16ï¼‰ï¼Œæ˜¾å­˜å ç”¨å°‘ï¼Œä½†å¯èƒ½æŸå¤±ç²¾åº¦æˆ–æ•°å€¼ä¸ç¨³å®šã€‚
   - è®¾ç½®å»ºè®®ï¼šGPU æ¨è torch.float16 æˆ– bfloat16ï¼ˆå…¼å®¹æ€§éœ€ç¡®è®¤ï¼‰ï¼›CPU é€šå¸¸ç”¨ float32ã€‚
3. **max_length**
   - æ¦‚å¿µï¼šç”Ÿæˆæ–‡æœ¬çš„æœ€å¤§é•¿åº¦ï¼ˆtoken æ•°é‡ï¼‰ã€‚
   - å½±å“ï¼šå€¼è¶Šå¤§ï¼Œç”Ÿæˆå†…å®¹è¶Šé•¿ï¼Œä½†é€Ÿåº¦è¶Šæ…¢ï¼Œä¸”å¯èƒ½é‡å¤æˆ–åç¦»ä¸»é¢˜ã€‚å€¼è¿‡å°å¯èƒ½å¯¼è‡´å›ç­”ä¸å®Œæ•´ã€‚
   - è®¾ç½®å»ºè®®ï¼šæ ¹æ®ä»»åŠ¡è°ƒæ•´ï¼šå¯¹è¯å»ºè®® 100-300ï¼Œé•¿æ–‡æœ¬ç”Ÿæˆå¯è®¾ 512-1024ï¼Œæ³¨æ„æ¨¡å‹æœ€å¤§é™åˆ¶ï¼ˆå¦‚ 4096ï¼‰ã€‚
4. **do_sample**
   - æ¦‚å¿µï¼šæ˜¯å¦å¯ç”¨é‡‡æ ·ç­–ç•¥ï¼ˆå¦‚ top_k, top_pï¼‰ã€‚è‹¥ä¸º Falseï¼Œåˆ™ä½¿ç”¨è´ªå¿ƒè§£ç ï¼ˆç¡®å®šæ€§å¼ºï¼‰ã€‚
   - å½±å“ï¼šTrueï¼šè¾“å‡ºå¤šæ ·åŒ–ï¼Œé€‚åˆåˆ›æ„ä»»åŠ¡ã€‚Falseï¼šè¾“å‡ºç¡®å®šæ€§å¼ºï¼Œé€‚åˆäº‹å®æ€§é—®é¢˜ã€‚
   - è®¾ç½®å»ºè®®ï¼šéœ€è¦å¤šæ ·æ€§æ—¶è®¾ä¸º Trueï¼Œéœ€å‡†ç¡®æ€§æ—¶è®¾ä¸º Falseã€‚
5. **top_k**
   - æ¦‚å¿µï¼šé‡‡æ ·æ—¶ä¿ç•™æ¦‚ç‡æœ€é«˜çš„å‰ k ä¸ª tokenã€‚
   - å½±å“ï¼šå€¼è¶Šå¤§ï¼ˆå¦‚ 100ï¼‰ï¼Œå€™é€‰ token å¤šï¼Œè¾“å‡ºå¤šæ ·ä½†å¯èƒ½ä¸ç›¸å…³ã€‚å€¼è¶Šå°ï¼ˆå¦‚ 10ï¼‰ï¼Œè¾“å‡ºæ›´ç¡®å®šä½†å¯èƒ½é‡å¤ã€‚
   - è®¾ç½®å»ºè®®ï¼šé€šå¸¸è®¾ä¸º 10-50ã€‚
6. **temperature**
   - æ¦‚å¿µï¼šæ§åˆ¶é‡‡æ ·éšæœºæ€§ï¼Œè°ƒæ•´æ¦‚ç‡åˆ†å¸ƒå¹³æ»‘åº¦ã€‚
   - å½±å“ï¼šå€¼å¤§ï¼ˆå¦‚ 1.5ï¼‰ï¼šè¾“å‡ºéšæœºæ€§é«˜ï¼Œå¯èƒ½ä¸è¿è´¯ã€‚å€¼å°ï¼ˆå¦‚ 0.1ï¼‰ï¼šè¾“å‡ºæ›´ç¡®å®šï¼Œä½†æ˜“é‡å¤ã€‚
   - è®¾ç½®å»ºè®®ï¼šå¹³è¡¡ç‚¹å¸¸ä¸º 0.7-1.0ï¼›éœ€åˆ›é€ æ€§æ—¶è°ƒé«˜ï¼ˆå¦‚ 0.9ï¼‰ï¼Œéœ€ä¿å®ˆæ—¶è°ƒä½ï¼ˆå¦‚ 0.3ï¼‰ã€‚
7. **top_pï¼ˆæ ¸é‡‡æ ·ï¼‰**
   - æ¦‚å¿µï¼šä»ç´¯ç§¯æ¦‚ç‡è¶…è¿‡é˜ˆå€¼ p çš„æœ€å° token é›†åˆä¸­é‡‡æ ·ã€‚
   - å½±å“ï¼šå€¼å¤§ï¼ˆå¦‚ 0.95ï¼‰ï¼šå€™é€‰ token å¤šï¼Œè¾“å‡ºå¤šæ ·ã€‚å€¼å°ï¼ˆå¦‚ 0.5ï¼‰ï¼šå€™é€‰ token å°‘ï¼Œè¾“å‡ºæ›´é›†ä¸­ã€‚
   - è®¾ç½®å»ºè®®ï¼šå¸¸ç”¨ 0.7-0.95ã€‚
8. **repetition_penalty**
   - æ¦‚å¿µï¼šæƒ©ç½šé‡å¤ token çš„æƒé‡ï¼ˆ>1.0 æ—¶æŠ‘åˆ¶é‡å¤ï¼Œ<1.0 æ—¶é¼“åŠ±é‡å¤ï¼‰ã€‚
   - å½±å“ï¼šå€¼å¤§ï¼ˆå¦‚ 2.0ï¼‰ï¼šå‡å°‘é‡å¤ï¼Œä½†å¯èƒ½ç”Ÿæˆä¸è‡ªç„¶å†…å®¹ã€‚å€¼å°ï¼ˆå¦‚ 1.0ï¼‰ï¼šæ— æƒ©ç½šï¼Œé»˜è®¤è¡Œä¸ºã€‚
   - è®¾ç½®å»ºè®®ï¼šé€šå¸¸è®¾ä¸º 1.0-1.2ï¼Œæ˜æ˜¾é‡å¤æ—¶å¯è®¾ 1.2-1.5ã€‚

#### 2.4.3 ä»£ç å°è£…
```python
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch


class DeepSeek:
    def __init__(self, model_path, device, torch_dtype):
        self.device = device  # è®¾å®šæ¨ç†è®¾å¤‡
        self.model = AutoModelForCausalLM.from_pretrained(
            model_path, torch_dtype=torch_dtype
        ).to(
            device
        )  # åŠ è½½æ¨¡å‹å¹¶ç§»åŠ¨åˆ°GPU
        self.tokenizer = AutoTokenizer.from_pretrained(model_path)  # åŠ è½½åˆ†è¯å™¨

    def inference(self, messages, gen_kwargs):
        inputs = self.tokenizer.apply_chat_template(
            messages,
            add_generation_prompt=True,
            tokenize=True,
            return_tensors="pt",
            return_dict=True,
        ).to(
            self.device
        )  # å°†è¾“å…¥æ•°æ®ç§»åŠ¨åˆ°GPU
        outputs = self.model.generate(**inputs, **gen_kwargs)  # ç”Ÿæˆè¾“å‡º
        outputs = outputs[:, inputs["input_ids"].shape[1] :]  # æˆªå–ç”Ÿæˆçš„è¾“å‡º
        result = self.tokenizer.decode(outputs[0], skip_special_tokens=True)  # è§£ç è¾“å‡º
        return result


if __name__ == "__main__":
    # ä¸€ï¼šè®¾å®šæ¨¡å‹è·¯å¾„å’Œè®¾å¤‡ï¼ŒåŠ è½½æ¨¡å‹
    model_path = r"./modeldir"  # æ›¿æ¢ä¸ºä½ çš„æ¨¡å‹è·¯å¾„
    device = "cuda:0" if torch.cuda.is_available() else "cpu"
    torch_dtype = torch.float16
    deepseek = DeepSeek(model_path, device, torch_dtype)

    # äºŒï¼šè®¾å®šæ¨ç†å‚æ•°ï¼Œæ¨ç†æ¶ˆæ¯
    gen_kwargs = {
        "max_length": 1024,  # ç”Ÿæˆçš„æœ€å¤§é•¿åº¦
        "do_sample": True,  # æ˜¯å¦ä½¿ç”¨æ¦‚ç‡é‡‡æ ·
        "top_k": 10,  # é‡‡æ ·æ—¶çš„å‰Kä¸ªå€™é€‰è¯ï¼Œè¶Šå¤§è¶Šéšæœº
        "temperature": 0.7,  # ç”Ÿæˆä¸°å¯Œæ€§ï¼Œè¶Šå¤§è¶Šæœ‰åˆ›é€ åŠ›
        "top_p": 0.8,  # é‡‡æ ·æ—¶çš„å‰Pä¸ªå€™é€‰è¯ï¼Œè¶Šå¤§è¶Šéšæœº
        "repetition_penalty": 1.2,
    }  # é‡å¤æƒ©ç½šç³»æ•°ï¼Œè¶Šå¤§è¶Šä¸å®¹æ˜“é‡å¤
    messages = [
        {"role": "system", "content": "ä½ æ˜¯ä¸€åä¹äºåŠ©äººçš„äººå·¥æ™ºèƒ½åŠ©æ‰‹"},
        {"role": "user", "content": "å†™ä¸€ä¸ªjsåˆ¤æ–­ç”¨æˆ·éªŒè¯ç ä»£ç "},
    ]  # å®šä¹‰æ¶ˆæ¯å†…å®¹
    result = deepseek.inference(messages, gen_kwargs)  # è°ƒç”¨æ¨ç†æ–¹æ³•
    print(result)  # æ‰“å°ç»“æœ

```

### 2.5 æµå¼æ¨ç†
```python
from transformers import AutoTokenizer, AutoModelForCausalLM, TextIteratorStreamer
import torch
from threading import Thread

class DeepSeek:
    def __init__(self, model_path, device, torch_dtype):
        self.device = device  # è®¾å®šæ¨ç†è®¾å¤‡
        self.model = AutoModelForCausalLM.from_pretrained(
            model_path, torch_dtype=torch_dtype
        ).to(
            device
        )  # åŠ è½½æ¨¡å‹å¹¶ç§»åŠ¨åˆ°GPU
        self.tokenizer = AutoTokenizer.from_pretrained(model_path)  # åŠ è½½åˆ†è¯å™¨

    def inference(self, messages, gen_kwargs):
        inputs = self.tokenizer.apply_chat_template(
            messages,
            add_generation_prompt=True,
            tokenize=True,
            return_tensors="pt",
            return_dict=True,
        ).to(
            self.device
        )  # å°†è¾“å…¥æ•°æ®ç§»åŠ¨åˆ°GPU
        streamer = TextIteratorStreamer(
            self.tokenizer, skip_special_tokens=True
        )  # åˆ›å»ºæµå¼è¾“å‡ºå¯¹è±¡
        generation_kwargs = dict(**inputs, **gen_kwargs, streamer=streamer)  # ç”Ÿæˆå‚æ•°
        thread = Thread(
            target=self.model.generate, kwargs=generation_kwargs
        )  # åˆ›å»ºçº¿ç¨‹
        thread.start()  # å¯åŠ¨çº¿ç¨‹è¿›è¡Œç”Ÿæˆ
        generated_text = ""  # åˆå§‹åŒ–ç”Ÿæˆæ–‡æœ¬
        for new_text in streamer:  # æµå¼è¾“å‡ºç”Ÿæˆçš„æ–‡æœ¬
            generated_text += new_text  # ç´¯åŠ ç”Ÿæˆçš„æ–‡æœ¬
            yield new_text  # é€æ­¥è¿”å›ç”Ÿæˆçš„æ–‡æœ¬


if __name__ == "__main__":
    # ä¸€ï¼šè®¾å®šæ¨¡å‹è·¯å¾„å’Œè®¾å¤‡ï¼ŒåŠ è½½æ¨¡å‹
    model_path = r"./modeldir"  # æ›¿æ¢ä¸ºä½ çš„æ¨¡å‹è·¯å¾„
    device = "cuda"  # æŒ‡å®šæ¨ç†è®¾å¤‡ä¸ºGPU
    torch_dtype = torch.float16
    deepseek = DeepSeek(model_path, device, torch_dtype)

    # äºŒï¼šè®¾å®šæ¨ç†å‚æ•°ï¼Œæ¨ç†æ¶ˆæ¯
    gen_kwargs = {
        "max_length": 1024,  # ç”Ÿæˆçš„æœ€å¤§é•¿åº¦
        "do_sample": True,  # æ˜¯å¦ä½¿ç”¨æ¦‚ç‡é‡‡æ ·
        "top_k": 10,  # é‡‡æ ·æ—¶çš„å‰Kä¸ªå€™é€‰è¯ï¼Œè¶Šå¤§è¶Šéšæœº
        "temperature": 0.7,  # ç”Ÿæˆä¸°å¯Œæ€§ï¼Œè¶Šå¤§è¶Šæœ‰åˆ›é€ åŠ›
        "top_p": 0.8,  # é‡‡æ ·æ—¶çš„å‰Pä¸ªå€™é€‰è¯ï¼Œè¶Šå¤§è¶Šéšæœº
        "repetition_penalty": 1.2,
    }  # é‡å¤æƒ©ç½šç³»æ•°ï¼Œè¶Šå¤§è¶Šä¸å®¹æ˜“é‡å¤
    messages = [
        {"role": "system", "content": "ä½ æ˜¯ä¸€åä¹äºåŠ©äººçš„äººå·¥æ™ºèƒ½åŠ©æ‰‹"},
        {"role": "user", "content": "è¯·ç®€è¦ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±"},
    ]  # å®šä¹‰æ¶ˆæ¯å†…å®¹
    response = deepseek.inference(messages, gen_kwargs)  # è°ƒç”¨æ¨ç†æ–¹æ³•
    result = ""  # åˆå§‹åŒ–ç»“æœ
    for chunk in response:  # æµå¼è¾“å‡ºç”Ÿæˆçš„æ–‡æœ¬
        result += chunk  # ç´¯åŠ ç”Ÿæˆçš„æ–‡æœ¬
        print(result)  # æ‰“å°ç»“æœ

```



## 3. qwen-0.5Bæœ¬åœ°éƒ¨ç½²

Qwenæ˜¯é˜¿é‡Œå·´å·´å…¬å¸ç ”å‘çš„å¤§æ¨¡å‹ï¼Œæˆ‘ä»¬ç»§ç»­ç”¨transformeråº“è¿›è¡Œæ¨ç†ï¼Œæ¨ç†æ¡†æ¶ä¸deepseekä¸€æ¨¡ä¸€æ ·ã€‚

- qwençš„huggingfaceåœ°å€ï¼š[Hugging Face Qwen](https://huggingface.co/Qwen/Qwen2.5-0.5B)

  <img src="media/image-20250420222438096.png" alt="image-20250420222438096" style="zoom:50%;" />

- ä¸‹è½½æ¨¡å‹æ–‡ä»¶ï¼š
   ```bash
   set HF_ENDPOINT=https://hf-mirror.com  # åŠ é€Ÿä¸‹è½½è®¾ç½®
   huggingface-cli download Qwen/Qwen2.5-0.5B --local-dir D:\code\code_py\medical_chat\qwen  # ä¸‹è½½æ¨¡å‹æ–‡ä»¶
   ```

â€‹	åç»­æ“ä½œå’ŒDeepSeekä¸€è‡´~



# ä¸‰ã€APPå¼€å‘å®æˆ˜

<img src="media/image-20250421213621677.png" alt="image-20250421213621677" style="zoom:50%;" />

## 1. å®¢æˆ·ç«¯ä»£ç 

éªŒè¯æ¯”è¾ƒå¥½ç”¨çš„markdownæ’ç‰ˆæ’ä»¶ï¼šhttps://ext.dcloud.net.cn/plugin?id=9437

<img src="media/image-20250421213728336.png" alt="image-20250421213728336" style="zoom:50%;" />

æˆ‘ä»¬è¿™é‡Œä½¿ç”¨socketå®æ—¶é€šä¿¡ï¼šhttps://zh.uniapp.dcloud.io/api/request/websocket.html#connectsocket

```vue
<template>
	<view class="container">
		<!-- é¡¶éƒ¨å¯¼èˆª -->
		<view class="header">
			<text class="menu-icon">â‰¡</text>
			<text class="title">æ–°å¯¹è¯</text>
			<text class="plus-icon">ğŸ’¬+</text>
		</view>

		<!-- ä¸»ä½“å†…å®¹ -->
		<view class="main-content">
			<image class="logo" src="/static/deepseek-logo.png" mode="aspectFit" />
			<text class="greeting">å—¨ï¼æˆ‘æ˜¯ <text class="deepseek-text">DeepSeek</text></text>
			<text class="desc">
				æˆ‘å¯ä»¥å¸®ä½ æœç´¢ã€ç­”ç–‘ã€å†™ä½œï¼Œ<br />
				è¯·æŠŠä½ çš„ä»»åŠ¡äº¤ç»™æˆ‘å§~
			</text>
		</view>

		<view class="chat-box">
			<view class="msg" v-for="(msg, index) in messages" :key="index">
				<zero-markdown-view :markdown="msg"></zero-markdown-view>
			</view>
			<zero-markdown-view :markdown="currentLine"></zero-markdown-view>
			<!-- <view>{{currentLine}}</view> -->
		</view>

		<!-- åº•éƒ¨è¾“å…¥æ¡† -->
		<view class="footerheight">

		</view>
		<view class="footer">
			<textarea class="input-box" v-model="inputMessage" placeholder="ç»™ DeepSeek å‘é€æ¶ˆæ¯" auto-height
				maxlength="200" />
			<button class="mini-btn mbtn" @click="sendMessage" type="warn" size="mini">å‘é€</button>
		</view>
	</view>
</template>

<style>
	.container {
		display: flex;
		flex-direction: column;
	}

	.header {
		display: flex;
		justify-content: space-between;
		align-items: center;
		padding: 20rpx 30rpx;
		font-size: 34rpx;
		font-weight: bold;
		color: #000000;
	}

	.menu-icon,
	.plus-icon {
		font-size: 38rpx;
	}

	.title {
		font-size: 36rpx;
	}

	.main-content {
		flex: 1;
		display: flex;
		flex-direction: column;
		justify-content: center;
		align-items: center;
		text-align: center;
		padding-bottom: 120rpx;
		/* é¢„ç•™åº•éƒ¨ç©ºé—´é¿å…å†…å®¹è¢«é®æŒ¡ */
	}

	.logo {
		width: 120rpx;
		height: 120rpx;
		margin-bottom: 40rpx;
	}

	.greeting {
		font-size: 32rpx;
		font-weight: bold;
		margin-bottom: 20rpx;
	}

	.deepseek-text {
		color: #2a5cff;
		font-weight: bold;
	}

	.desc {
		font-size: 28rpx;
		color: #666666;
		line-height: 1.8;
	}

	.footer {
		position: fixed;
		bottom: 0;
		left: 0;
		right: 0;
		background-color: #f5f5f5;
		padding: 20rpx;
		box-shadow: 0 -4rpx 10rpx rgba(0, 0, 0, 0.05);
	}

	.footerheight {
		min-height: 120rpx;
	}

	.input-box {
		width: 680rpx;
		min-height: 80rpx;
		font-size: 28rpx;
		color: #333;
		background-color: #ffffff;
		padding: 20rpx;
		border-radius: 20rpx;
	}

	.mbtn {
		margin-top: 10rpx;
	}
</style>

<script>
	import mpHtml from 'mp-html/dist/uni-app/components/mp-html/mp-html'
	export default {
		components: {
			mpHtml
		},
		data() {
			return {
				socket: null,
				inputMessage: '',
				messages: [],
				currentLine: '',
				isclose: false
			}
		},
		onLoad() {
			this.initWebSocket();
		},
		methods: {
			initWebSocket() {
				this.socketTask = uni.connectSocket({
					url: 'ws://10.71.21.8:8000/ws/chat/',
					success: () => {
						console.log("1. WebSocket åˆ›å»ºæˆåŠŸ");
					}
				});

				this.socketTask.onOpen(() => {
					console.log("2. WebSocket è¿æ¥å·²æ‰“å¼€");
				});

				this.socketTask.onMessage((res) => {
					const data = JSON.parse(res.data);
					if (data.type === 'start') {
						this.currentLine = data.content;
					} else if (data.type === 'stream') {
						this.currentLine += data.content;
					} else if (data.type === 'end') {
						this.messages.push(this.currentLine);
						console.log(this.messages)
						this.currentLine = '';
					}
					// åœ¨é¡µé¢æ–¹æ³•ä¸­è°ƒç”¨
					uni.pageScrollTo({
						scrollTop: 10000000, // è®¾ç½®ä¸€ä¸ªè¶³å¤Ÿå¤§çš„å€¼
						duration: 300 // åŠ¨ç”»æ—¶é—´
					});

				});

				this.socketTask.onClose(() => {
					console.log("3. WebSocket å·²å…³é—­");
					this.isclose = true
				});
			},

			sendMessage() {
				if (this.isclose) {
					this.initWebSocket();
					this.isclose = false
				}
				if (!this.inputMessage) return;

				this.messages.push('ä½ ï¼š' + this.inputMessage);
				this.socketTask.send({
					data: JSON.stringify({
						message: this.inputMessage
					})
				});
				this.inputMessage = '';
			}
		}
	}
</script>
```



## 2. æœåŠ¡å™¨ç«¯ä»£ç 

Django æœ¬èº«ä¸ç›´æ¥æ”¯æŒ WebSocketï¼Œå› ä¸ºå®ƒæ˜¯ä¸ºä¼ ç»Ÿçš„ HTTP è¯·æ±‚-å“åº”æ¨¡å‹è®¾è®¡çš„ã€‚ä½†ä½ å¯ä»¥é€šè¿‡ **Django + ASGI + Channels** æ¥å®ç° WebSocket é€šä¿¡ã€‚

### 2.1 åŸºæœ¬ç»„ä»¶ä»‹ç»

| ç»„ä»¶     | ä½œç”¨è¯´æ˜                                                |
| -------- | ------------------------------------------------------- |
| Django   | æä¾›åŸºæœ¬çš„ MVC æ¡†æ¶å’Œè·¯ç”±ã€ORMã€è§†å›¾ç­‰                  |
| ASGI     | å¼‚æ­¥æœåŠ¡å™¨ç½‘å…³æ¥å£ï¼Œæ”¯æŒ WebSocket                      |
| Channels | Django å®˜æ–¹æ¨èçš„å¼‚æ­¥é€šä¿¡åº“ï¼Œæ”¯æŒ WebSocketã€åå°ä»»åŠ¡ç­‰ |
| Daphne   | Channels æ¨èä½¿ç”¨çš„ ASGI æœåŠ¡å™¨                         |

### 2.2 ä¾èµ–åŒ…å®‰è£…

```bash
pip install channels
```



### 2.3 é¡¹ç›®é…ç½®

é€šè¿‡ä¿®æ”¹é…ç½®ï¼Œè®©æˆ‘ä»¬çš„DjangoæœåŠ¡å™¨æ”¯æŒsocketé€šä¿¡ã€‚

- ä¿®æ”¹ settings.py

  ```python
  # æŒ‡å®š ASGI åº”ç”¨  MyLLMï¼šæ”¹æˆè‡ªå·±çš„åº”ç”¨
  ASGI_APPLICATION = "MyLLM.asgi.application"
  ```

- ä¿®æ”¹ MyLLM\asgi.py

  ==MyLLMæ”¹æˆè‡ªå·±çš„å·¥ç¨‹å==
  
  ```python
  import os
  from channels.routing import ProtocolTypeRouter, URLRouter
  from channels.auth import AuthMiddlewareStack
  from django.core.asgi import get_asgi_application
  import myapp.routing  # ä½ è‡ªå®šä¹‰çš„ routing.py
  
  os.environ.setdefault("DJANGO_SETTINGS_MODULE", "MyLLM.settings")
  
  application = ProtocolTypeRouter(
      {
          "http": get_asgi_application(),
          "websocket": AuthMiddlewareStack(
              URLRouter(myapp.routing.websocket_urlpatterns)
          ),
      }
  )
  ```

### 2.4 è·¯ç”±å®šä¹‰

  å½“ç„¶äº†ï¼Œè¿™é‡Œè¦å…ˆ==åˆ›å»ºä¸€ä¸ªåº”ç”¨myapp~==

  ```python 
  # routing.py
  
  from django.urls import re_path
  from . import consumers
  
  websocket_urlpatterns = [
      re_path(r"ws/chat/$", consumers.ChatStreamConsumer.as_asgi()),
  ]
  ```

  ### 2.5 Consumerå®šä¹‰

ç”¨äºå¤„ç†websocket

  ```python
  # consumers.py
  import torch
  import os
  from .deepseek import DeepSeek
  
  CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))
  # è·¯å¾„æ ¹æ®å®é™…æƒ…å†µè°ƒæ•´ä»¥ä¸‹
  model_path = os.path.join(CURRENT_DIR, "../../", "deepseekModel")
  # ä¸€ï¼šåŠ è½½æ¨¡å‹
  device = "cuda:0" if torch.cuda.is_available() else "cpu"
  
  
  from channels.generic.websocket import AsyncWebsocketConsumer
  import json
  import asyncio
  
  
  class ChatStreamConsumer(AsyncWebsocketConsumer):
      async def connect(self):
          print("1. ebSocketè¿æ¥å¼€å§‹")
          await self.accept()
          print("2. WebSocketè¿æ¥å·²æ¥æ”¶")
  
      async def disconnect(self, close_code):
          print("3. è¿æ¥æ–­å¼€")
  
      async def receive(self, text_data):
          print("4. æ”¶åˆ°æ¶ˆæ¯ï¼š", text_data)
          data = json.loads(text_data)
          user_message = data.get("message", "")
  
          # æ¨¡æ‹Ÿ AI å›å¤å‰çš„â€œAIæ€è€ƒä¸­â€æç¤º
          await self.send(
              text_data=json.dumps({"type": "start", "content": "AIæ­£åœ¨æ€è€ƒä¸­...\n"})
          )
  
          # ä¸€ï¼šè®¾å®šæ¨¡å‹è·¯å¾„å’Œè®¾å¤‡ï¼ŒåŠ è½½æ¨¡å‹
          torch_dtype = torch.float16
          deepseek = DeepSeek(model_path, device, torch_dtype)
  
          # äºŒï¼šè®¾å®šæ¨ç†å‚æ•°ï¼Œæ¨ç†æ¶ˆæ¯
          gen_kwargs = {
              "max_length": 3072,  # ç”Ÿæˆçš„æœ€å¤§é•¿åº¦
              "do_sample": True,  # æ˜¯å¦ä½¿ç”¨æ¦‚ç‡é‡‡æ ·
              "top_k": 10,  # é‡‡æ ·æ—¶çš„å‰Kä¸ªå€™é€‰è¯ï¼Œè¶Šå¤§è¶Šéšæœº
              "temperature": 0.7,  # ç”Ÿæˆä¸°å¯Œæ€§ï¼Œè¶Šå¤§è¶Šæœ‰åˆ›é€ åŠ›
              "top_p": 0.8,  # é‡‡æ ·æ—¶çš„å‰Pä¸ªå€™é€‰è¯ï¼Œè¶Šå¤§è¶Šéšæœº
              "repetition_penalty": 1.2,
          }  # é‡å¤æƒ©ç½šç³»æ•°ï¼Œè¶Šå¤§è¶Šä¸å®¹æ˜“é‡å¤
          messages = [
              {"role": "system", "content": "ä½ æ˜¯ä¸€åä¹äºåŠ©äººçš„äººå·¥æ™ºèƒ½åŠ©æ‰‹"},
              {"role": "user", "content": user_message},
          ]  # å®šä¹‰æ¶ˆæ¯å†…å®¹
          response = deepseek.inference(messages, gen_kwargs)  # è°ƒç”¨æ¨ç†æ–¹æ³•
          for chunk in response:  # æµå¼è¾“å‡ºç”Ÿæˆçš„æ–‡æœ¬
              await self.send(text_data=json.dumps({"type": "stream", "content": chunk}))
  
          await self.send(text_data=json.dumps({"type": "end"}))  # è¡¨ç¤ºç»“æŸ
  ```

  è¿™é‡Œç”¨åˆ°DeepSeekç±»ï¼š

  ```python
  from transformers import AutoTokenizer, AutoModelForCausalLM, TextIteratorStreamer
  from threading import Thread
  class DeepSeek:
      def __init__(self, model_path, device, torch_dtype):
          self.device = device  # è®¾å®šæ¨ç†è®¾å¤‡
          self.model = AutoModelForCausalLM.from_pretrained(
              model_path, torch_dtype=torch_dtype
          ).to(
              device
          )  # åŠ è½½æ¨¡å‹å¹¶ç§»åŠ¨åˆ°GPU
          self.tokenizer = AutoTokenizer.from_pretrained(model_path)  # åŠ è½½åˆ†è¯å™¨
  
      def inference(self, messages, gen_kwargs):
          inputs = self.tokenizer.apply_chat_template(
              messages,
              add_generation_prompt=True,
              tokenize=True,
              return_tensors="pt",
              return_dict=True,
          ).to(
              self.device
          )  # å°†è¾“å…¥æ•°æ®ç§»åŠ¨åˆ°GPU
          streamer = TextIteratorStreamer(
              self.tokenizer, skip_special_tokens=False
          )  # åˆ›å»ºæµå¼è¾“å‡ºå¯¹è±¡
          generation_kwargs = dict(**inputs, **gen_kwargs, streamer=streamer)  # ç”Ÿæˆå‚æ•°
          thread = Thread(
              target=self.model.generate, kwargs=generation_kwargs
          )  # åˆ›å»ºçº¿ç¨‹
          thread.start()  # å¯åŠ¨çº¿ç¨‹è¿›è¡Œç”Ÿæˆ
          for new_text in streamer:  # æµå¼è¾“å‡ºç”Ÿæˆçš„æ–‡æœ¬
              yield new_text  # é€æ­¥è¿”å›ç”Ÿæˆçš„æ–‡æœ¬
  
  ```

  

### 2.6 æœåŠ¡å™¨å¯åŠ¨

è¯¥è£…çš„åŒ…è¿˜æ˜¯è¦è£…ï¼š

```bash
pip install websockets
pip install uvicorn
```

å¯åŠ¨ï¼šè¿™æ ·æ‰æ”¯æŒwebsocket

```bash
uvicorn --port 8000 --host 192.168.2.21 aiserver.asgi:application --reload
uvicorn --port 8000 --host 192.168.43.237 aiserver.asgi:application --reload
uvicorn --port 8000 --host 127.0.0.1 MyLLM.asgi:application --reload
```



## 3. è·¨åŸŸå¤„ç†

<img src="media/image-20250422094336716.png" alt="image-20250422094336716" style="zoom: 50%;" />

åœ¨ Django ä¸­å¤„ç†è·¨åŸŸé—®é¢˜ï¼Œé€šå¸¸ä¼šä½¿ç”¨ **CORS**ï¼ˆè·¨åŸŸèµ„æºå…±äº«ï¼‰æ¥è§£å†³ã€‚ä½ å¯ä»¥ä½¿ç”¨ `django-cors-headers` åº“æ¥å¤„ç†è¿™ä¸ªé—®é¢˜ã€‚ä»¥ä¸‹æ˜¯å¦‚ä½•åœ¨ Django é¡¹ç›®ä¸­é…ç½® CORS çš„æ­¥éª¤ï¼š

1. **å®‰è£… `django-cors-headers` åº“ï¼š**

   é¦–å…ˆï¼Œä½ éœ€è¦å®‰è£… `django-cors-headers` åº“ï¼Œå¯ä»¥é€šè¿‡ `pip` å®‰è£…ï¼š

   ```bash
   pip install django-cors-headers
   ```

2. **åœ¨ `settings.py` ä¸­è¿›è¡Œé…ç½®ï¼š**

   ä¹‹åï¼Œéœ€è¦åœ¨ Django çš„ `settings.py` ä¸­è¿›è¡Œç›¸å…³é…ç½®ï¼š

   - å°† `'corsheaders'` æ·»åŠ åˆ° `INSTALLED_APPS` ä¸­ï¼š

     ```python
     INSTALLED_APPS = [
         # å…¶ä»– app
         'corsheaders',
     ]
     ```

   - å°† `'corsheaders.middleware.CorsMiddleware'` æ·»åŠ åˆ° `MIDDLEWARE` ä¸­ï¼š

     ```python
     MIDDLEWARE = [
         # å…¶ä»– middleware
         'corsheaders.middleware.CorsMiddleware',
     ]
     ```

   - è®¾ç½®å…è®¸è·¨åŸŸçš„æºï¼ˆå¯ä»¥å…è®¸æ‰€æœ‰åŸŸï¼Œä¹Ÿå¯ä»¥æŒ‡å®šç‰¹å®šåŸŸï¼‰ï¼š

     å…è®¸æ‰€æœ‰åŸŸçš„è·¨åŸŸè®¿é—®ï¼š

     ```python
     CORS_ALLOW_ALL_ORIGINS = True
     ```

     æˆ–è€…ï¼ŒæŒ‡å®šå…è®¸è·¨åŸŸçš„åŸŸåï¼š

     ```python
     CORS_ALLOWED_ORIGINS = [
         "http://example.com",
         "http://localhost:3000",  # æœ¬åœ°å¼€å‘æ—¶å¯èƒ½éœ€è¦
     ]
     ```

3. **å…¶ä»–é…ç½®ï¼ˆå¯é€‰ï¼‰ï¼š**

   ä½ è¿˜å¯ä»¥æ ¹æ®éœ€è¦æ·»åŠ æ›´å¤šé…ç½®ï¼Œä¾‹å¦‚ï¼š

   - å…è®¸è·¨åŸŸæ—¶æºå¸¦è®¤è¯ä¿¡æ¯ï¼ˆå¦‚ Cookieï¼‰ï¼š

     ```python
     CORS_ALLOW_CREDENTIALS = True
     ```

   - å…è®¸ç‰¹å®šçš„ HTTP æ–¹æ³•ï¼š

     ```python
     CORS_ALLOWED_METHODS = [
         'GET',
         'POST',
         'PUT',
         'DELETE',
         'PATCH',
     ]
     ```

   - å…è®¸ç‰¹å®šçš„è¯·æ±‚å¤´ï¼š

     ```python
     CORS_ALLOW_HEADERS = [
         'content-type',
         'authorization',
         'x-requested-with',
     ]
     ```

4. **é‡å¯æœåŠ¡å™¨ï¼š**

   å®Œæˆä¸Šè¿°é…ç½®åï¼Œé‡å¯ Django æœåŠ¡å™¨ï¼Œä½¿å…¶ç”Ÿæ•ˆã€‚



## 4. æ‰‹æœºå¦‚ä½•è®¿é—®

æ‰‹æœºè¿æ¥ç”µè„‘çš„çƒ­ç‚¹ï¼šå½¢æˆä¸€ä¸ªå±€åŸŸç½‘ï¼Œç”µè„‘ç«¯çš„é˜²ç«å¢™æœ€å¥½å…³æ‰



==æ³¨æ„ï¼šä¸€å®šä¸è¦ç”¨127.0.0.1ã€localhost==
