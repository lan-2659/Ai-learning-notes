# 一、MoE

**MoE，Mixture of Experts，专家混合模型**，是一种稀疏激活（sparse activation）的神经网络结构。它通过为输入动态选择一小部分子网络参与计算，从而在保持大模型容量的同时，显著降低计算开销。

> 🎯 本质：使用 gating 网络为输入选择最适合的 K 个专家进行计算。



## 1. 组成部分

| 组件                | 说明                                             |
| ------------------- | ------------------------------------------------ |
| Experts             | 专家：多个子网络，每个拥有独立参数               |
| Router / Gate       | 路由器：对输入打分，选择 Top-k 个专家            |
| Top-k Gating        | 选择机制：选择得分最高的 $K$ 个专家              |
| Load Balancing Loss | 负载均衡损失：防止专家使用不均，增强模型泛化能力 |



## 2. 工作流程

1. 输入 token 特征 → 送入 gating 网络
2. 得到每个专家的权重分数，通常使用 softmax
3. 选择 Top-k 个专家
4. 将 token 分发给这 $K$ 个专家并加权求和
5. 输出合并后继续后续计算



## 3. 优点与挑战

口水化的东西~

### 3.1 优点

- 超大容量，参数量远超 dense 模型
- 推理效率高，每次只激活部分专家
- 可以专门化不同专家应对不同类型任务/输入

### 3.2 挑战

- 路由器训练不稳定
- 容易导致专家不均衡（部分专家常被选中）
- 分布式部署复杂，通信开销较高



## 4. 负载均衡损失

Load Balancing Loss 是一种正则化机制，目标是让 MoE 中的专家模块在训练和推理中使用更均衡，从而提升模型的效率和泛化能力。

### 4.1 专家使用不均衡

在 MoE 的训练过程中，如果没有额外约束：

- 某些专家总是被路由器选中
- 其他专家几乎从不被使用（参数无法学习）

这样会导致：

- 专家冗余（浪费参数）
- 泛化能力变差（只有少部分专家在工作）
- 容易训练不稳定（少数专家压力过大）



而Load Balancing Loss 的作用就是鼓励模型更均匀地使用所有专家。它并不会直接影响主任务（如文本生成、分类），而是作为辅助目标，让 gating 网络**不要过于偏向某几个专家**。



### 4.2 数学公式

假设：

- $G(x) \in \mathbb{R}^E$：$gating$ 网络对输入 $x$ 输出的 softmax 分布（$E$ 为专家个数）
- $\text{mean}(G(x))$：平均下来，每个专家被选中的概率
- $\text{auxiliary loss} = E \cdot \sum_{i=1}^E \text{mean}(G_i(x))^2$

这个式子鼓励所有专家的使用频率接近平均。



### 4.3 案例助解

==场景假设==

- 假设你的 MoE 模型中有 4 个专家：$E=4$
- 你有一个小批量的输入：$batch=3$ 个 $token$
- Gating 网络对每个 token 预测专家概率分布如下：softmax 输出

| Token\专家 | Expert 0 | Expert 1 | Expert 2 | Expert 3 |
| ---------- | -------- | -------- | -------- | -------- |
| Token 1    | 0.70     | 0.10     | 0.10     | 0.10     |
| Token 2    | 0.80     | 0.05     | 0.10     | 0.05     |
| Token 3    | 0.60     | 0.20     | 0.10     | 0.10     |



#### 4.3.1 计算平均概率

计算所有 $token$ 在每个专家上的被选中平均概率：
$$
\text{mean}(G_i) = \frac{1}{3} \sum_{j=1}^3 G_{j,i}
$$
==计算结果：==

| 专家     | 计算                     | 平均概率 |
| -------- | ------------------------ | -------- |
| Expert 0 | (0.70 + 0.80 + 0.60) / 3 | 0.70     |
| Expert 1 | (0.10 + 0.05 + 0.20) / 3 | 0.1167   |
| Expert 2 | (0.10 + 0.10 + 0.10) / 3 | 0.10     |
| Expert 3 | (0.10 + 0.05 + 0.10) / 3 | 0.0833   |



#### 4.3.2 计算负载均衡损失

公式是：
$$
\text{Loss} = E \times \sum_{i=1}^E \text{mean}(G_i)^2
$$
带入数值：
$$
4 \times (0.7^2 + 0.1167^2 + 0.1^2 + 0.0833^2) = 4 \times (0.49 + 0.0136 + 0.01 + 0.0069) = 4 \times 0.5205 = 2.082
$$

#### 4.3.3 对比理想情况

如果 gating 网络分配均匀：
$$
\text{mean}(G_i) = 0.25, \quad \forall i
$$
计算：
$$
4 \times \sum_{i=1}^4 (0.25^2) = 4 \times 4 \times 0.0625 = 1.0
$$

#### 4.3.4  解释

- **损失值越大表示分配越不均匀**，越容易被训练惩罚
- 在当前示例中，损失是 2.08，明显大于均匀分配的 1.0
- 模型会倾向调整 gating 网络，让平均概率更均匀分布，降低该辅助损失

